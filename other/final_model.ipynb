{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final result on complete data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign BDEs and other descriptors to data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from mystructmatch_utils import add_ref_idx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    '/hits/fast/mbm/treydewk/optimized_test/BDEs_compl.txt',\n",
    "    sep = '\\t\\t',\n",
    "    names = ['names', 'BDE_H', 'BDE_G', 'charge', 'pdb', 'pdb_H', 'ref_comp'],\n",
    "    engine='python'\n",
    ")\n",
    "\n",
    "### read in manually assigned alternative indices\n",
    "alt_ind = pd.read_csv(\n",
    "    '/hits/fast/mbm/treydewk/optimized_test/alt_ind_compl.txt',\n",
    "    sep = '\\t\\t',\n",
    "    names = ['names', 'alt_idx'], engine='python'\n",
    ")\n",
    "### the indices returned by the matching function are zero-indexed, whereas the PDB indices are one-indexed\n",
    "for i in range(0,len(alt_ind['alt_idx'])):\n",
    "    if not alt_ind.iloc[i,1] is None:\n",
    "        ele_list = alt_ind.iloc[i,1].split(sep=',')\n",
    "        for j,k in enumerate(ele_list):\n",
    "            ele_list[j] = int(k)-1 \n",
    "        alt_ind.iloc[i,1] = ele_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Manual structure matching for Gly_x-1-9.pdb\n",
    "termini_df = data.iloc[[192,194]] # 192 -> 8, 194 -> 10\n",
    "termini_df = termini_df.reset_index(drop=True)\n",
    "ref_termini = [Path('/hits/basement/mbm/riedmiki/structures/KR0008/reference_structures/Gly_x-1-9.pdb'), Path('/hits/basement/mbm/riedmiki/structures/KR0008/reference_structures/Gly_x-1-9.pdb')]\n",
    "idx_termini = [8,10]\n",
    "idx_termini_list = [[8,],[10,]]\n",
    "matched = pd.DataFrame(zip(ref_termini, idx_termini, idx_termini_list), columns=['ref', 'ref_idx', 'alt_idx'])\n",
    "termini_df = termini_df.join(matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to pad missing BDEs\n",
    "N_term_BDE = float(data[data['names'] == 'Nterminus_amino']['BDE_H'])\n",
    "N_term_BDE_row = data[data['names'] == 'Nterminus_amino']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop data for neutral arginine [8-13], aspartic acid [19-21], backbone [22], C termini [23-24],\n",
    "# glutamic acid [54-57], cationic histidine [59-64], His_pi_beta [66]\n",
    "# uncharged lysine [126-131], \n",
    "# N termini [136-138], pyd crosslinks [152-168], acetylated and N-amino formylated termini [192-195]\n",
    "\n",
    "######################################\n",
    "\n",
    "# hlknl crosslinks [75-86],\n",
    "data = data.drop(index=[\n",
    "    8, 9, 10, 11, 12, 13, 19, 20, 21, 22, 23, 24, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 66, 75, 76, 77, 78, 79, 80, 81, 82,\n",
    "    83, 84, 85, 86, 93, 94, 126, 127, 128, 129, 130, 131, 136, 137, 138, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161,\n",
    "    162, 163, 164, 165, 166, 167, 168, 192, 193, 194, 195\n",
    "])\n",
    "alt_ind = alt_ind.drop(index=[\n",
    "    8, 9, 10, 11, 12, 13, 19, 20, 21, 22, 23, 24, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 66, 75, 76, 77, 78, 79, 80, 81, 82,\n",
    "    83, 84, 85, 86, 93, 94, 126, 127, 128, 129, 130, 131, 136, 137, 138, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161,\n",
    "    162, 163, 164, 165, 166, 167, 168, 192, 193, 194, 195\n",
    "])\n",
    "data = data.reset_index(drop=True)\n",
    "alt_ind = alt_ind.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### average BDEs for cis and trans conformers of proline and hydroxyproline\n",
    "# Hyp\n",
    "alpha_cis = data[data['names'] == 'Hyp_cis_alpha']\n",
    "alpha_trans = data[data['names'] == 'Hyp_trans_alpha']\n",
    "pyrr3_cis = data[data['names'] == 'Hyp_cis_pyrr3']\n",
    "pyrr3_trans = data[data['names'] == 'Hyp_trans_pyrr3']\n",
    "pyrr4_onC_cis = data[data['names'] == 'Hyp_cis_pyrr4_onC']\n",
    "pyrr4_onC_trans = data[data['names'] == 'Hyp_trans_pyrr4_onC']\n",
    "pyrr4_onO_cis = data[data['names'] == 'Hyp_cis_pyrr4_onO']\n",
    "pyrr4_onO_trans = data[data['names'] == 'Hyp_trans_pyrr4_onO']\n",
    "pyrr5_cis = data[data['names'] == 'Hyp_cis_pyrr5']\n",
    "pyrr5_trans = data[data['names'] == 'Hyp_trans_pyrr5']\n",
    "\n",
    "alpha_idx = alpha_cis.index[0]\n",
    "pyrr3_idx = pyrr3_cis.index[0]\n",
    "pyrr4_onC_idx = pyrr4_onC_cis.index[0]\n",
    "pyrr4_onO_idx = pyrr4_onO_cis.index[0]\n",
    "pyrr5_idx = pyrr5_cis.index[0]\n",
    "\n",
    "data.iloc[alpha_idx,0] = 'Hyp_alpha'\n",
    "data.iloc[pyrr3_idx,0] = 'Hyp_pyrr3'\n",
    "data.iloc[pyrr4_onC_idx,0] = 'Hyp_pyrr4_onC'\n",
    "data.iloc[pyrr4_onO_idx,0] = 'Hyp_pyrr4_onO'\n",
    "data.iloc[pyrr5_idx,0] = 'Hyp_pyrr5'\n",
    "\n",
    "data.iloc[alpha_idx,1] = (alpha_cis.iloc[0,1] + alpha_trans.iloc[0,1])/2\n",
    "data.iloc[pyrr3_idx,1] = (pyrr3_cis.iloc[0,1] + pyrr3_trans.iloc[0,1])/2\n",
    "data.iloc[pyrr4_onC_idx,1] = (pyrr4_onC_cis.iloc[0,1] + pyrr4_onC_trans.iloc[0,1])/2\n",
    "data.iloc[pyrr4_onO_idx,1] = (pyrr4_onO_cis.iloc[0,1] + pyrr4_onO_trans.iloc[0,1])/2\n",
    "data.iloc[pyrr5_idx,1] = (pyrr5_cis.iloc[0,1] + pyrr5_trans.iloc[0,1])/2\n",
    "\n",
    "data.iloc[alpha_idx,2] = (alpha_cis.iloc[0,2] + alpha_trans.iloc[0,2])/2\n",
    "data.iloc[pyrr3_idx,2] = (pyrr3_cis.iloc[0,2] + pyrr3_trans.iloc[0,2])/2\n",
    "data.iloc[pyrr4_onC_idx,2] = (pyrr4_onC_cis.iloc[0,2] + pyrr4_onC_trans.iloc[0,2])/2\n",
    "data.iloc[pyrr4_onO_idx,2] = (pyrr4_onO_cis.iloc[0,2] + pyrr4_onO_trans.iloc[0,2])/2\n",
    "data.iloc[pyrr5_idx,2] = (pyrr5_cis.iloc[0,2] + pyrr5_trans.iloc[0,2])/2\n",
    "\n",
    "data = data.drop(index=[\n",
    "    alpha_trans.index[0], pyrr3_trans.index[0], pyrr4_onC_trans.index[0], pyrr4_onO_trans.index[0], pyrr5_trans.index[0]\n",
    "])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "alt_ind = alt_ind.drop(index=[\n",
    "    alpha_trans.index[0], pyrr3_trans.index[0], pyrr4_onC_trans.index[0], pyrr4_onO_trans.index[0], pyrr5_trans.index[0]\n",
    "])\n",
    "alt_ind = alt_ind.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pro\n",
    "alpha_cis = data[data['names'] == 'Pro_cis_alpha']\n",
    "alpha_trans = data[data['names'] == 'Pro_trans_alpha']\n",
    "pyrr3_cis = data[data['names'] == 'Pro_cis_pyrr3']\n",
    "pyrr3_trans = data[data['names'] == 'Pro_trans_pyrr3']\n",
    "pyrr4_cis = data[data['names'] == 'Pro_cis_pyrr4']\n",
    "pyrr4_trans = data[data['names'] == 'Pro_trans_pyrr4']\n",
    "pyrr5_cis = data[data['names'] == 'Pro_cis_pyrr5']\n",
    "pyrr5_trans = data[data['names'] == 'Pro_trans_pyrr5']\n",
    "\n",
    "alpha_idx = alpha_cis.index[0]\n",
    "pyrr3_idx = pyrr3_cis.index[0]\n",
    "pyrr4_idx = pyrr4_cis.index[0]\n",
    "pyrr5_idx = pyrr5_cis.index[0]\n",
    "\n",
    "data.iloc[alpha_idx,0] = 'Pro_alpha'\n",
    "data.iloc[pyrr3_idx,0] = 'Pro_pyrr3'\n",
    "data.iloc[pyrr4_idx,0] = 'Pro_pyrr4'\n",
    "data.iloc[pyrr5_idx,0] = 'Pro_pyrr5'\n",
    "\n",
    "data.iloc[alpha_idx,1] = (alpha_cis.iloc[0,1] + alpha_trans.iloc[0,1])/2\n",
    "data.iloc[pyrr3_idx,1] = (pyrr3_cis.iloc[0,1] + pyrr3_trans.iloc[0,1])/2\n",
    "data.iloc[pyrr4_idx,1] = (pyrr4_cis.iloc[0,1] + pyrr4_trans.iloc[0,1])/2\n",
    "data.iloc[pyrr5_idx,1] = (pyrr5_cis.iloc[0,1] + pyrr5_trans.iloc[0,1])/2\n",
    "\n",
    "data.iloc[alpha_idx,2] = (alpha_cis.iloc[0,2] + alpha_trans.iloc[0,2])/2\n",
    "data.iloc[pyrr3_idx,2] = (pyrr3_cis.iloc[0,2] + pyrr3_trans.iloc[0,2])/2\n",
    "data.iloc[pyrr4_idx,2] = (pyrr4_cis.iloc[0,2] + pyrr4_trans.iloc[0,2])/2\n",
    "data.iloc[pyrr5_idx,2] = (pyrr5_cis.iloc[0,2] + pyrr5_trans.iloc[0,2])/2\n",
    "\n",
    "data = data.drop(index=[\n",
    "    alpha_trans.index[0], pyrr3_trans.index[0], pyrr4_trans.index[0], pyrr5_trans.index[0]\n",
    "])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "alt_ind = alt_ind.drop(index=[\n",
    "    alpha_trans.index[0], pyrr3_trans.index[0], pyrr4_trans.index[0], pyrr5_trans.index[0]\n",
    "])\n",
    "alt_ind = alt_ind.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure:  Ala_alpha--> Ala-0.pdb\n",
      "Structure:  Ala_beta--> Ala-0.pdb\n",
      "Structure:  Arg_c1_alpha--> Arg_c1-0.pdb\n",
      "Structure:  Arg_c1_beta--> Arg_c1-0.pdb\n",
      "Structure:  Arg_c1_gamma--> Arg_c1-0.pdb\n",
      "Structure:  Arg_c1_delta--> Arg_c1-0.pdb\n",
      "Structure:  Arg_c1_epsilon--> Arg_c1-0.pdb\n",
      "Structure:  Arg_c1_guan--> Arg_c1-0.pdb\n",
      "Structure:  Asn_alpha--> Asn-0.pdb\n",
      "Structure:  Asn_beta--> Asn-0.pdb\n",
      "Structure:  Asn_amide--> Asn-0.pdb\n",
      "Structure:  Asp_c-1_alpha--> Asp_c-1-0.pdb\n",
      "Structure:  Asp_c-1_beta--> Asp_c-1-0.pdb\n",
      "Structure:  Cys_alpha--> Cys-0.pdb\n",
      "Structure:  Cys_beta--> Cys-0.pdb\n",
      "Structure:  Cys_sulphur--> Cys-0.pdb\n",
      "Structure:  Dopa_alpha--> Dop-0.pdb\n",
      "Structure:  Dopa_beta--> Dop-0.pdb\n",
      "Structure:  Dopa_ortho1--> Dop-0.pdb\n",
      "Structure:  Dopa_OH_meta--> Dop-0.pdb\n",
      "Structure:  Dopa_OH_para--> Dop-0.pdb\n",
      "Structure:  Dopa_meta2--> Dop-0.pdb\n",
      "Structure:  Dopa_ortho2--> Dop-0.pdb\n",
      "Structure:  Dopa_c-1meta_alpha--> Dop_c-1_b-0.pdb\n",
      "Structure:  Dopa_c-1meta_beta--> Dop_c-1_b-0.pdb\n",
      "Structure:  Dopa_c-1meta_ortho1--> Dop_c-1_b-0.pdb\n",
      "Structure:  Dopa_c-1meta_OH--> Dop_c-1_b-0.pdb\n",
      "Structure:  Dopa_c-1meta_meta2--> Dop_c-1_b-0.pdb\n",
      "Structure:  Dopa_c-1meta_ortho2--> Dop_c-1_b-0.pdb\n",
      "Structure:  Dopa_c-1para_alpha--> Dop_c-1_a-0.pdb\n",
      "Structure:  Dopa_c-1para_beta--> Dop_c-1_a-0.pdb\n",
      "Structure:  Dopa_c-1para_ortho1--> Dop_c-1_a-0.pdb\n",
      "Structure:  Dopa_c-1para_OH--> Dop_c-1_a-0.pdb\n",
      "Structure:  Dopa_c-1para_meta2--> Dop_c-1_a-0.pdb\n",
      "Structure:  Dopa_c-1para_ortho2--> Dop_c-1_a-0.pdb\n",
      "Structure:  Gln_alpha--> Gln-0.pdb\n",
      "Structure:  Gln_beta--> Gln-0.pdb\n",
      "Structure:  Gln_gamma--> Gln-0.pdb\n",
      "Structure:  Gln_amide--> Gln-0.pdb\n",
      "Structure:  Glu_c-1_alpha--> Glu_c-1-0.pdb\n",
      "Structure:  Glu_c-1_beta--> Glu_c-1-0.pdb\n",
      "Structure:  Glu_c-1_gamma--> Glu_c-1-0.pdb\n",
      "Structure:  Gly_alpha--> Gly-0.pdb\n",
      "Structure:  His_pi_alpha--> His_x-4-9.pdb\n",
      "Structure:  His_pi_imidazole1--> His_x-4-9.pdb\n",
      "Structure:  His_pi_imidazole2--> His_x-4-9.pdb\n",
      "Structure:  His_pi_imidazole4--> His_x-4-9.pdb\n",
      "Structure:  His_tau_alpha--> His-0.pdb\n",
      "Structure:  His_tau_beta--> His-0.pdb\n",
      "Structure:  His_tau_imidazole2--> His-0.pdb\n",
      "Structure:  His_tau_imidazole3--> His-0.pdb\n",
      "Structure:  His_tau_imidazole4--> His-0.pdb\n",
      "Structure:  Hyl_c1_alpha--> Hyl_c1-0.pdb\n",
      "Structure:  Hyl_c1_beta--> Hyl_c1-0.pdb\n",
      "Structure:  Hyl_c1_gamma--> Hyl_c1-0.pdb\n",
      "Structure:  Hyl_c1_delta_onC--> Hyl_c1-0.pdb\n",
      "Structure:  Hyl_c1_delta_onO--> Hyl_c1-0.pdb\n",
      "Structure:  Hyl_c1_epsilon--> Hyl_c1-0.pdb\n",
      "Structure:  Hyl_beta--> Hyl-0.pdb\n",
      "Structure:  Hyl_gamma--> Hyl-0.pdb\n",
      "Structure:  Hyl_delta_onC--> Hyl-0.pdb\n",
      "Structure:  Hyl_delta_onO--> Hyl-0.pdb\n",
      "Structure:  Hyl_epsilon--> Hyl-0.pdb\n",
      "Structure:  Hyl_amine--> Hyl-0.pdb\n",
      "Structure:  Hyp_alpha--> Hyp-0.pdb\n",
      "Structure:  Hyp_pyrr3--> Hyp-0.pdb\n",
      "Structure:  Hyp_pyrr4_onC--> Hyp-0.pdb\n",
      "Structure:  Hyp_pyrr4_onO--> Hyp-0.pdb\n",
      "Structure:  Hyp_pyrr5--> Hyp-0.pdb\n",
      "Structure:  Ile_alpha--> Ile-0.pdb\n",
      "Structure:  Ile_beta--> Ile-0.pdb\n",
      "Structure:  Ile_gammaMe--> Ile-0.pdb\n",
      "Structure:  Ile_gammaEt--> Ile-0.pdb\n",
      "Structure:  Ile_delta--> Ile-0.pdb\n",
      "Structure:  Leu_alpha--> Leu-0.pdb\n",
      "Structure:  Leu_beta--> Leu-0.pdb\n",
      "Structure:  Leu_gamma--> Leu-0.pdb\n",
      "Structure:  Leu_delta--> Leu-0.pdb\n",
      "Structure:  Lys_c1_alpha--> Lys_c1-0.pdb\n",
      "Structure:  Lys_c1_beta--> Lys_c1-0.pdb\n",
      "Structure:  Lys_c1_gamma--> Lys_c1-0.pdb\n",
      "Structure:  Lys_c1_delta--> Lys_c1-0.pdb\n",
      "Structure:  Lys_c1_epsilon--> Lys_c1-0.pdb\n",
      "Structure:  Lys_c1_amine--> Lys_c1-0.pdb\n",
      "Structure:  Met_alpha--> Met-0.pdb\n",
      "Structure:  Met_beta--> Met-0.pdb\n",
      "Structure:  Met_gamma--> Met-0.pdb\n",
      "Structure:  Met_epsilon--> Met-0.pdb\n",
      "Structure:  Phe_alpha--> Phe-0.pdb\n",
      "Structure:  Phe_beta--> Phe-0.pdb\n",
      "Structure:  Phe_ortho--> Phe-0.pdb\n",
      "Structure:  Phe_meta--> Phe-0.pdb\n",
      "Structure:  Phe_para--> Phe-0.pdb\n",
      "Structure:  Pro_alpha--> Pro-0.pdb\n",
      "Structure:  Pro_pyrr3--> Pro-0.pdb\n",
      "Structure:  Pro_pyrr4--> Pro-0.pdb\n",
      "Structure:  Pro_pyrr5--> Pro-0.pdb\n",
      "Structure:  Ser_alpha--> Ser-0.pdb\n",
      "Structure:  Ser_beta_onO--> Ser-0.pdb\n",
      "Structure:  Ser_beta_onC--> Ser-0.pdb\n",
      "Structure:  Thr_alpha--> Thr-0.pdb\n",
      "Structure:  Thr_beta_onO--> Thr-0.pdb\n",
      "Structure:  Thr_beta_onC--> Thr-0.pdb\n",
      "Structure:  Thr_gamma--> Thr-0.pdb\n",
      "Structure:  Trp_alpha--> Trp-0.pdb\n",
      "Structure:  Trp_beta--> Trp-0.pdb\n",
      "Structure:  Trp_indol1--> Trp-0.pdb\n",
      "Structure:  Trp_indol2--> Trp-0.pdb\n",
      "Structure:  Trp_indol5--> Trp-0.pdb\n",
      "Structure:  Trp_indol6--> Trp-0.pdb\n",
      "Structure:  Trp_indol7--> Trp-0.pdb\n",
      "Structure:  Trp_indol8--> Trp-0.pdb\n",
      "Structure:  Tyr_alpha--> Tyr-0.pdb\n",
      "Structure:  Tyr_beta--> Tyr-0.pdb\n",
      "Structure:  Tyr_ortho--> Tyr-0.pdb\n",
      "Structure:  Tyr_meta--> Tyr-0.pdb\n",
      "Structure:  Tyr_hydroxy--> Tyr-0.pdb\n",
      "Structure:  Val_alpha--> Val-0.pdb\n",
      "Structure:  Val_beta--> Val-0.pdb\n",
      "Structure:  Val_gamma--> Val-0.pdb\n"
     ]
    }
   ],
   "source": [
    "results = data.join(data.apply(add_ref_idx, axis=1, result_type=\"expand\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2930661/2628140588.py:3: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(termini_df)\n"
     ]
    }
   ],
   "source": [
    "alt_ind.drop(columns=['names',], inplace=True)\n",
    "results = results.join(alt_ind)\n",
    "results = results.append(termini_df)\n",
    "results = results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pickle('BDE_df_compl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tidy_idx = pd.read_pickle('/hits/basement/mbm/riedmiki/structures/KR0008/df_tidy_pckl_220401_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_stem = pd.DataFrame([str(ref.resolve())[:-6] for ref in results['ref']], columns = ['ref_stem',])\n",
    "results = results.join(ref_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_rad = []; name_H = []; ref_comp_rad = []; ref_comp_H = []\n",
    "BDEs_sorted_rad = []; BDEs_sorted_H = []; BDEs_G_sorted_rad = []; BDEs_G_sorted_H = []\n",
    "\n",
    "for rad_ref, rad_ref_idx in zip(df_tidy_idx['rad_ref'], df_tidy_idx['rad_ref_idx']):\n",
    "    ref_path = str(rad_ref.resolve())[:-6]\n",
    "    found = results[results['ref_stem']==ref_path]\n",
    "    to_drop = []\n",
    "    for i,l in zip(found.index, found['alt_idx']):\n",
    "        if int(rad_ref_idx) not in l:\n",
    "            to_drop.append(i)\n",
    "    found.drop(to_drop, inplace=True)\n",
    "    if found.shape[0]>0:\n",
    "        idx = found.index[0]\n",
    "        name_rad.append(results.iloc[idx,0])\n",
    "        BDEs_sorted_rad.append(results.iloc[idx,1])\n",
    "        BDEs_G_sorted_rad.append(results.iloc[idx,2])\n",
    "        ref_comp_rad.append(results.iloc[idx,6])\n",
    "    else:\n",
    "        name_rad.append(np.nan)\n",
    "        BDEs_sorted_rad.append(np.nan)\n",
    "        BDEs_G_sorted_rad.append(np.nan)\n",
    "        ref_comp_rad.append(np.nan)\n",
    "\n",
    "for h_ref, h_ref_idx in zip(df_tidy_idx['h_ref'], df_tidy_idx['h_ref_idx']):\n",
    "    ref_path = str(h_ref.resolve())[:-6]\n",
    "    found = results[results['ref_stem']==ref_path]\n",
    "    to_drop = []\n",
    "    for i,l in zip(found.index, found['alt_idx']):\n",
    "        if int(h_ref_idx) not in l:\n",
    "            to_drop.append(i)\n",
    "    found.drop(to_drop, inplace=True)\n",
    "    if found.shape[0]>0:\n",
    "        idx = found.index[0]\n",
    "        name_H.append(results.iloc[idx,0])\n",
    "        BDEs_sorted_H.append(results.iloc[idx,1])\n",
    "        BDEs_G_sorted_H.append(results.iloc[idx,2])\n",
    "        ref_comp_H.append(results.iloc[idx,6])\n",
    "    else:\n",
    "        name_H.append(np.nan)\n",
    "        BDEs_sorted_H.append(np.nan)\n",
    "        BDEs_G_sorted_H.append(np.nan)\n",
    "        ref_comp_H.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDEs_df = pd.DataFrame(\n",
    "    zip(name_rad, name_H, ref_comp_rad, ref_comp_H, BDEs_sorted_rad, BDEs_sorted_H, BDEs_G_sorted_rad, BDEs_G_sorted_H),\n",
    "    columns = ['rad_chem_name', 'H_chem_name', 'rad_ref_comp', 'H_ref_comp', 'rad_BDE', 'H_BDE', 'rad_BDE_G', 'H_BDE_G']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reactions for which a BDE is missing involve reactions with an amine group after a backbone \n",
    "# break, so let's pad these values with the BDE for a neutral N terminus\n",
    "BDEs_df = BDEs_df.fillna(N_term_BDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = df_tidy_idx.join(BDEs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete.to_pickle('data_complete_compl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BDE_data = pd.read_pickle('BDE_df_compl')\n",
    "N_term_BDE_row = N_term_BDE_row.join(\n",
    "    pd.DataFrame({'ref': 'ref', 'ref_idx': 1, 'alt_idx': [1]})\n",
    ")\n",
    "BDE_data = pd.concat([BDE_data, N_term_BDE_row])\n",
    "BDE_data = BDE_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_BDE = complete['rad_BDE'].to_list()\n",
    "H_BDE = complete['H_BDE'].to_list()\n",
    "\n",
    "rad_PDB = []\n",
    "for rad in rad_BDE:\n",
    "    idx = BDE_data[BDE_data['BDE_H'] == rad].index[0]\n",
    "    rad_PDB.append(BDE_data.iloc[idx, 4])\n",
    "\n",
    "H_PDB = []\n",
    "for H in H_BDE:\n",
    "    idx = BDE_data[BDE_data['BDE_H'] == H].index[0]\n",
    "    H_PDB.append(BDE_data.iloc[idx, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_df = pd.read_csv('/hits/fast/mbm/treydewk/optimized_test/descriptors.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mordred errors\n",
    "to_drop = []\n",
    "for column in descriptors_df.columns:\n",
    "    if isinstance(descriptors_df[column].to_list()[1], str):\n",
    "        to_drop.append(column)\n",
    "del to_drop[to_drop.index('names')], to_drop[to_drop.index('pdb')], to_drop[to_drop.index('SMILES')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_df.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### average descriptors for cis and trans conformers of proline and hydroxyproline\n",
    "# Hyp\n",
    "alpha_cis = descriptors_df[descriptors_df['names'] == 'hyp_cis_pyrr2_1']\n",
    "alpha_trans = descriptors_df[descriptors_df['names'] == 'Hyp_trans_pyrr2_1']\n",
    "pyrr3_cis = descriptors_df[descriptors_df['names'] == 'hyp_cis_pyrr3_1']\n",
    "pyrr3_trans = descriptors_df[descriptors_df['names'] == 'Hyp_trans_pyrr3_1']\n",
    "pyrr4_onC_cis = descriptors_df[descriptors_df['names'] == 'hyp_cis_pyrr4_onC_1']\n",
    "pyrr4_onC_trans = descriptors_df[descriptors_df['names'] == 'Hyp_trans_pyrr4_onC_1']\n",
    "pyrr4_onO_cis = descriptors_df[descriptors_df['names'] == 'hyp_cis_pyrr4_onO_1']\n",
    "pyrr4_onO_trans = descriptors_df[descriptors_df['names'] == 'Hyp_trans_pyrr4_onO_1']\n",
    "pyrr5_cis = descriptors_df[descriptors_df['names'] == 'hyp_cis_pyrr5_1']\n",
    "pyrr5_trans = descriptors_df[descriptors_df['names'] == 'Hyp_trans_pyrr5_1']\n",
    "\n",
    "alpha_idx = alpha_cis.index[0]\n",
    "pyrr3_idx = pyrr3_cis.index[0]\n",
    "pyrr4_onC_idx = pyrr4_onC_cis.index[0]\n",
    "pyrr4_onO_idx = pyrr4_onO_cis.index[0]\n",
    "pyrr5_idx = pyrr5_cis.index[0]\n",
    "\n",
    "for i in range(4,descriptors_df.shape[1]):\n",
    "    descriptors_df.iloc[alpha_idx,i] = (alpha_cis.iloc[0,i] + alpha_trans.iloc[0,i])/2\n",
    "    descriptors_df.iloc[pyrr3_idx,i] = (pyrr3_cis.iloc[0,i] + pyrr3_trans.iloc[0,i])/2\n",
    "    descriptors_df.iloc[pyrr4_onC_idx,i] = (pyrr4_onC_cis.iloc[0,i] + pyrr4_onC_trans.iloc[0,i])/2\n",
    "    descriptors_df.iloc[pyrr4_onO_idx,i] = (pyrr4_onO_cis.iloc[0,i] + pyrr4_onO_trans.iloc[0,i])/2\n",
    "    descriptors_df.iloc[pyrr5_idx,i] = (pyrr5_cis.iloc[0,i] + pyrr5_trans.iloc[0,i])/2\n",
    "\n",
    "descriptors_df = descriptors_df.drop(index=[\n",
    "    alpha_trans.index[0], pyrr3_trans.index[0], pyrr4_onC_trans.index[0], pyrr4_onO_trans.index[0], pyrr5_trans.index[0]\n",
    "])\n",
    "descriptors_df = descriptors_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pro\n",
    "alpha_cis = descriptors_df[descriptors_df['names'] == 'pro_cis_pyrr2_1']\n",
    "alpha_trans = descriptors_df[descriptors_df['names'] == 'Pro_trans_pyrr2_1']\n",
    "pyrr3_cis = descriptors_df[descriptors_df['names'] == 'pro_cis_pyrr3_1']\n",
    "pyrr3_trans = descriptors_df[descriptors_df['names'] == 'Pro_trans_pyrr3_1']\n",
    "pyrr4_cis = descriptors_df[descriptors_df['names'] == 'pro_cis_pyrr4_1']\n",
    "pyrr4_trans = descriptors_df[descriptors_df['names'] == 'Pro_trans_pyrr4_1']\n",
    "pyrr5_cis = descriptors_df[descriptors_df['names'] == 'pro_cis_pyrr5_1']\n",
    "pyrr5_trans = descriptors_df[descriptors_df['names'] == 'Pro_trans_pyrr5_1']\n",
    "\n",
    "alpha_idx = alpha_cis.index[0]\n",
    "pyrr3_idx = pyrr3_cis.index[0]\n",
    "pyrr4_idx = pyrr4_cis.index[0]\n",
    "pyrr5_idx = pyrr5_cis.index[0]\n",
    "\n",
    "for i in range(4,descriptors_df.shape[1]):\n",
    "    descriptors_df.iloc[alpha_idx,i] = (alpha_cis.iloc[0,i] + alpha_trans.iloc[0,i])/2\n",
    "    descriptors_df.iloc[pyrr3_idx,i] = (pyrr3_cis.iloc[0,i] + pyrr3_trans.iloc[0,i])/2\n",
    "    descriptors_df.iloc[pyrr4_idx,i] = (pyrr4_cis.iloc[0,i] + pyrr4_trans.iloc[0,i])/2\n",
    "    descriptors_df.iloc[pyrr5_idx,i] = (pyrr5_cis.iloc[0,i] + pyrr5_trans.iloc[0,i])/2\n",
    "\n",
    "descriptors_df = descriptors_df.drop(index=[\n",
    "    alpha_trans.index[0], pyrr3_trans.index[0], pyrr4_trans.index[0], pyrr5_trans.index[0]\n",
    "])\n",
    "descriptors_df = descriptors_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_sorted_rad = pd.DataFrame()\n",
    "descriptors_sorted_H = pd.DataFrame()\n",
    "\n",
    "for pdb in rad_PDB:\n",
    "    idx = descriptors_df[descriptors_df['pdb']==pdb].index[0]\n",
    "    descriptors_sorted_rad = descriptors_sorted_rad.append(descriptors_df.iloc[idx])\n",
    "\n",
    "for pdb in H_PDB:\n",
    "    idx = descriptors_df[descriptors_df['pdb']==pdb].index[0]\n",
    "    descriptors_sorted_H = descriptors_sorted_H.append(descriptors_df.iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_sorted_rad = descriptors_sorted_rad.drop(columns = [descriptors_sorted_rad.columns[0], 'names'])\n",
    "descriptors_sorted_H = descriptors_sorted_H.drop(columns = [descriptors_sorted_H.columns[0], 'names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in descriptors_sorted_rad.columns:\n",
    "    descriptors_sorted_rad.rename(columns = {column: '{}_rad'.format(column)}, inplace = True)\n",
    "\n",
    "for column in descriptors_sorted_H.columns:\n",
    "    descriptors_sorted_H.rename(columns = {column: '{}_H'.format(column)}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = complete.index\n",
    "descriptors_sorted_rad = descriptors_sorted_rad.set_index(indices)\n",
    "descriptors_sorted_H = descriptors_sorted_H.set_index(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = complete.join(descriptors_sorted_rad)\n",
    "final_results = final_results.join(descriptors_sorted_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.to_pickle('data_complete_w_descriptors_020422')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kgcnn.utils.adj import coordinates_to_distancematrix, define_adjacency_from_distance, distance_to_gauss_basis, get_angle_indices, sort_edge_indices\n",
    "from ase.io import read\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('/hits/basement/mbm/riedmiki/structures/KR0008/')\n",
    "\n",
    "se_folders = list((root/'traj').glob('batch*/se'))\n",
    "\n",
    "se_folders = se_folders + [root/'start_end_prod_1', root/'start_end_prod_2', root/'start_end_prod_3', root/'start_end_prod_4', root/'start_end_prod_6',\n",
    "root/'start_end_prod_7', root/'start_end_prod_8', root/'start_end_prod_9', root/'start_end_prod_10', root/'start_end_prod_11', root/'start_end_prod_intra_2']\n",
    "\n",
    "pdb_files_all = [f for f in root.glob('**/*.pdb') if (re.search('_1.pdb', f.name) or re.search('_2.pdb', f.name))]\n",
    "\n",
    "data = pd.read_pickle('data_complete_w_descriptors_020422')\n",
    "hashes_se = []\n",
    "for hash1, hash2 in zip(data['hash_u1'], data['hash_u2']):\n",
    "    hashes_se.append([str(hash1) + '_' + str(hash2) + '_1.pdb', str(hash1) + '_' + str(hash2) + '_2.pdb'])\n",
    "\n",
    "pdb_files = [f for f in pdb_files_all if f.name in list(chain.from_iterable(hashes_se))]\n",
    "directions = data['reaction'].to_list()\n",
    "\n",
    "stems = []\n",
    "for file in pdb_files:\n",
    "    stems.append(file.stem)\n",
    "\n",
    "pdb_files_sorted_start = []\n",
    "pdb_files_sorted_end = []\n",
    "\n",
    "for hash, direction in zip(hashes_se, directions):\n",
    "    if direction == 1:\n",
    "        idx = stems.index(hash[0][:-4])\n",
    "        pdb_files_sorted_start.append(pdb_files[idx])\n",
    "        idx = stems.index(hash[1][:-4])\n",
    "        pdb_files_sorted_end.append(pdb_files[idx])\n",
    "\n",
    "    if direction == 2:\n",
    "        idx = stems.index(hash[1][:-4])\n",
    "        pdb_files_sorted_start.append(pdb_files[idx])\n",
    "        idx = stems.index(hash[0][:-4])\n",
    "        pdb_files_sorted_end.append(pdb_files[idx])\n",
    "\n",
    "all_nodes_start = []\n",
    "all_pos_start = []\n",
    "\n",
    "for file_start, file_end in zip(pdb_files_sorted_start, pdb_files_sorted_end):\n",
    "\n",
    "    mol_start = read(str(file_start.resolve()))\n",
    "    mol_end = read(str(file_end.resolve()))\n",
    "\n",
    "    an_start = mol_start.get_atomic_numbers()\n",
    "    pos_start = mol_start.positions\n",
    "    pos_end = mol_end.positions\n",
    "\n",
    "    # reacting H atom in its final position\n",
    "    nodes_start = np.concatenate((np.array([0]), an_start), axis=0)\n",
    "    pos_start_compl = np.concatenate((np.array([pos_end[0]]), pos_start), axis=0)\n",
    "\n",
    "    all_nodes_start.append(nodes_start)\n",
    "    all_pos_start.append(pos_start_compl)\n",
    "\n",
    "# create distance matrices, adjacency matrices and edge indices\n",
    "dist_mat_start = [coordinates_to_distancematrix(x) for x in all_pos_start]\n",
    "adj_mat_start = [define_adjacency_from_distance(x)[0] for x in dist_mat_start]\n",
    "edge_idx_start = [define_adjacency_from_distance(x)[1] for x in dist_mat_start]\n",
    "\n",
    "graph_input = pd.DataFrame(\n",
    "    zip(all_nodes_start, all_pos_start, edge_idx_start),\n",
    "    columns = [\n",
    "        'nodes_start', 'pos_start', 'egde_idx_start'\n",
    "    ]\n",
    ")\n",
    "\n",
    "graph_input.to_pickle('graph_input_pickled_020422')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from kgcnn.utils.data import ragged_tensor_from_nested_numpy\n",
    "from kgcnn.layers.conv.painn_conv import PAiNNUpdate, EquivariantInitialize\n",
    "from kgcnn.layers.conv.painn_conv import PAiNNconv\n",
    "from kgcnn.layers.geom import NodeDistanceEuclidean, BesselBasisLayer, EdgeDirectionNormalized, CosCutOffEnvelope, NodePosition\n",
    "from kgcnn.layers.modules import LazyAdd, OptionalInputEmbedding\n",
    "from kgcnn.layers.mlp import MLP\n",
    "from kgcnn.layers.pooling import PoolingNodes\n",
    "from kgcnn.layers.gather import GatherNodes, GatherNodesIngoing\n",
    "from kgcnn.utils.adj import coordinates_to_distancematrix, define_adjacency_from_distance, distance_to_gauss_basis, get_angle_indices, sort_edge_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def painn(inputs=[{\"shape\": (None,), \"name\": \"node_attributes\", \"dtype\": \"float32\", \"ragged\": True},\n",
    "                            {\"shape\": (None, 3), \"name\": \"node_coordinates\", \"dtype\": \"float32\", \"ragged\": True},\n",
    "                            {\"shape\": (None, 2), \"name\": \"edge_indices\", \"dtype\": \"int64\", \"ragged\": True},\n",
    "                            {'shape': (None, 2), 'name': 'node_radical_input', 'dtype': 'int64', 'ragged': True},\n",
    "                            {'shape': (None, 1), 'name': 'edge_radical_input', 'dtype': 'int64', 'ragged': True},\n",
    "                            {'shape': (25,), 'name': 'input_desc', 'dtype': 'float32', 'ragged': True}],\n",
    "               input_embedding={\"node\": {\"input_dim\": 95, \"output_dim\": 128}},\n",
    "               bessel_basis={\"num_radial\": 20, \"cutoff\": 5.0, \"envelope_exponent\": 5},\n",
    "               depth=2,\n",
    "               pooling_args={\"pooling_method\": \"sum\"},\n",
    "               conv_args={\"units\": 128, \"cutoff\": None, \"conv_pool\": \"sum\"},\n",
    "               update_args={\"units\": 128},\n",
    "               output_mlp={\"use_bias\": [True, True, True, True, True],\n",
    "                                \"units\": [512, 256, 128, 64, 1], \"activation\": [\"swish\", \"swish\", \"swish\", \"swish\", \"linear\"]}\n",
    "               ):\n",
    "    \"\"\"Make PAiNN graph network via functional API.\n",
    "\n",
    "    Args:\n",
    "        inputs : list\n",
    "            List of dictionaries unpacked in :obj:`tf.keras.layers.Input`. Order must match model definition.\n",
    "        input_embedding : dict\n",
    "            Dictionary of embedding arguments for nodes etc. unpacked in `Embedding` layers.\n",
    "        bessel_basis : dict\n",
    "            Dictionary of layer arguments unpacked in final `BesselBasisLayer` layer.\n",
    "        depth : int\n",
    "            Number of graph embedding units or depth of the network.\n",
    "        pooling_args : dict\n",
    "            Dictionary of layer arguments unpacked in `PoolingNodes` layer.\n",
    "        conv_args : dict\n",
    "            Dictionary of layer arguments unpacked in `PAiNNconv` layer.\n",
    "        update_args : dict\n",
    "            Dictionary of layer arguments unpacked in `PAiNNUpdate` layer.\n",
    "        output_mlp : dict\n",
    "            Dictionary of layer arguments unpacked in the final classification `MLP` layer block.\n",
    "            Defines number of model outputs and activation.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.models.Model\n",
    "    \"\"\"\n",
    "\n",
    "    # Make input\n",
    "    node_input = tf.keras.layers.Input(**inputs[0])\n",
    "    xyz_input = tf.keras.layers.Input(**inputs[1])\n",
    "    bond_index_input = tf.keras.layers.Input(**inputs[2])\n",
    "    node_radical_index = tf.keras.layers.Input(**inputs[3])\n",
    "    edge_radical_index = tf.keras.layers.Input(**inputs[4])\n",
    "    eri_n = node_radical_index\n",
    "    eri_e = edge_radical_index\n",
    "    descriptors = tf.keras.layers.Input(**inputs[5])\n",
    "    z = OptionalInputEmbedding(**input_embedding['node'],\n",
    "                               use_embedding=len(inputs[0]['shape']) < 2)(node_input)\n",
    "\n",
    "    equiv_input = EquivariantInitialize(dim=3)(z)\n",
    "\n",
    "    edi = bond_index_input\n",
    "    x = xyz_input\n",
    "    v = equiv_input\n",
    "\n",
    "    pos1, pos2 = NodePosition()([x, edi])\n",
    "    rij = EdgeDirectionNormalized()([pos1, pos2])\n",
    "    d = NodeDistanceEuclidean()([pos1, pos2])\n",
    "    env = CosCutOffEnvelope(conv_args[\"cutoff\"])(d)\n",
    "    rbf = BesselBasisLayer(**bessel_basis)(d)\n",
    "\n",
    "    for i in range(depth):\n",
    "        # Message\n",
    "        ds, dv = PAiNNconv(**conv_args)([z, v, rbf, env, rij, edi])\n",
    "        z = LazyAdd()([z, ds])\n",
    "        v = LazyAdd()([v, dv])\n",
    "        # Update\n",
    "        ds, dv = PAiNNUpdate(**update_args)([z, v])\n",
    "        z = LazyAdd()([z, ds])\n",
    "        v = LazyAdd()([v, dv])\n",
    "    n = z\n",
    "\n",
    "    n_radical = GatherNodes()([n, eri_n])\n",
    "    e_radical = GatherNodesIngoing()([rbf, eri_e])\n",
    "\n",
    "    rad_embedd = tf.keras.layers.Concatenate(axis=-1)([n_radical, e_radical])\n",
    "    rad_embedd = PoolingNodes(**pooling_args)(rad_embedd)\n",
    "\n",
    "    out = tf.keras.layers.Concatenate()([rad_embedd, descriptors])\n",
    "\n",
    "    initial_output = MLP(**output_mlp)(out)\n",
    "    concat_input = tf.keras.layers.Concatenate(axis=-1)([out, initial_output])\n",
    "    main_output = MLP(**output_mlp)(concat_input)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[\n",
    "        node_input, xyz_input, bond_index_input, node_radical_index, edge_radical_index, descriptors\n",
    "    ], outputs=main_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painn = painn()\n",
    "\n",
    "data = pd.read_pickle('data_complete_w_descriptors_020422')\n",
    "target = data['Ea'].to_numpy()\n",
    "descriptors = data[[\n",
    "    'translation', 'rad_BDE', 'H_BDE', 'max_spin_rad',\n",
    "    'mull_charge_rad', 'bur_vol_iso_rad', 'nBase_rad', 'SpMax_A_rad',\n",
    "    'ATSC2s_rad', 'ATSC1Z_rad', 'ATSC2i_rad', 'NdNH_rad', 'SMR_VSA4_rad',\n",
    "    'max_spin_H', 'mull_charge_H', 'bur_vol_iso_H', 'nBase_H', 'SpMax_A_H',\n",
    "    'ATSC2s_H', 'ATSC1Z_H', 'ATSC2i_H', 'GATS2dv_H', \n",
    "    'BCUTdv-1h_H', 'SMR_VSA4_H', 'VSA_EState7_H'\n",
    "]]\n",
    "del data\n",
    "\n",
    "descriptors = np.array(descriptors)\n",
    "\n",
    "graph_input = pd.read_pickle('graph_input_pickled_020422')\n",
    "nodes = graph_input['nodes_start'].to_numpy()\n",
    "pos = graph_input['pos_start'].to_numpy()\n",
    "edge_idx = graph_input['egde_idx_start'].to_numpy()\n",
    "\n",
    "dist_mat_start = [coordinates_to_distancematrix(x) for x in pos]\n",
    "adj_mat_start = [define_adjacency_from_distance(x)[0] for x in dist_mat_start]\n",
    "edge_idx = [x if x[0,1]==1 else sort_edge_indices(np.concatenate([np.array([[0, 1], [1,0]]), x], axis=0)) for x in edge_idx]\n",
    "edge_idx = np.array(edge_idx)\n",
    "node_radical_index = [np.array([[0, 1]]) for _ in edge_idx]  \n",
    "edge_radical_index = [np.array([[0]]) for _ in node_radical_index]\n",
    "node_radical_index = np.array(node_radical_index)\n",
    "edge_radical_index = np.array(edge_radical_index)\n",
    "del graph_input\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.001, decay_steps=15122*0.8/256*1000, decay_rate=1, staircase=False\n",
    ")\n",
    "optimizer=tf.keras.optimizers.Adam(lr_schedule)\n",
    "callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "painn.compile(loss='mae', optimizer=optimizer)\n",
    "\n",
    "targets_train, targets_val, nodes_train, nodes_val, pos_train, pos_val, \\\n",
    "    edge_idx_train, edge_idx_val, eri_n_train, eri_n_val, \\\n",
    "        eri_e_train, eri_e_val, descriptors_train, descriptors_val = train_test_split(\n",
    "            target, nodes, pos, edge_idx, node_radical_index, edge_radical_index, descriptors,\n",
    "            test_size = 0.2, random_state=1\n",
    "        )\n",
    "\n",
    "# normalize descriptors\n",
    "means = descriptors_train.mean(axis=0)\n",
    "stds = descriptors_train.std(axis=0)\n",
    "epsilon = 1e-7\n",
    "descriptors_train = (descriptors_train - means) / (stds + epsilon)\n",
    "descriptors_val = (descriptors_val - means) / (stds + epsilon)\n",
    "\n",
    "nodes_train, nodes_val = ragged_tensor_from_nested_numpy(nodes_train), ragged_tensor_from_nested_numpy(nodes_val)\n",
    "pos_train, pos_val = ragged_tensor_from_nested_numpy(pos_train), ragged_tensor_from_nested_numpy(pos_val)\n",
    "edge_idx_train, edge_idx_val = ragged_tensor_from_nested_numpy(edge_idx_train), ragged_tensor_from_nested_numpy(edge_idx_val)\n",
    "eri_n_train, eri_n_val = ragged_tensor_from_nested_numpy(eri_n_train), ragged_tensor_from_nested_numpy(eri_n_val)\n",
    "eri_e_train, eri_e_val = ragged_tensor_from_nested_numpy(eri_e_train), ragged_tensor_from_nested_numpy(eri_e_val)\n",
    "\n",
    "data_train = nodes_train, pos_train, edge_idx_train, eri_n_train, eri_e_train, descriptors_train\n",
    "data_val = nodes_val, pos_val, edge_idx_val, eri_n_val, eri_e_val, descriptors_val\n",
    "\n",
    "history = painn.fit(data_train, targets_train,\n",
    "            batch_size=128,\n",
    "            epochs=5000,\n",
    "            verbose=0,\n",
    "            validation_data=(data_val, targets_val),\n",
    "            callbacks=callbacks)\n",
    "\n",
    "train_score = painn.evaluate(data_train, targets_train, verbose=0, batch_size=128)\n",
    "val_score = painn.evaluate(data_val, targets_val, verbose=0, batch_size=128)\n",
    "print('Train score:', train_score, '\\nValidation score:', val_score)\n",
    "\n",
    "# painn.save('saved_models/painn_compl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Batch size: 64, 5000 epochs.\n",
    "\n",
    "Train score: 1.147171974182129 \n",
    "\n",
    "Validation score: 4.122030735015869\n",
    "\n",
    "------------------------------------------------------------------------------------------------\n",
    "Batch size: 128, 5000 epochs.\n",
    "\n",
    "Train score: 0.8191611766815186 \n",
    "\n",
    "Validation score: 4.069589138031006\n",
    "\n",
    "------------------------------------------------------------------------------------------------\n",
    "Batch size: 256, 2000 epochs\n",
    "\n",
    "Train score: 1.3984484672546387 \n",
    "\n",
    "Validation score: 4.312300682067871"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9810cd3559edf93e350c9ff83472b07b449301d37c5da9aaba0d8181928edeb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv_up': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
