{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 20:04:05.848819: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "import keras_tuner as kt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load input data ###\n",
    "data = pd.read_pickle('data_complete_final')\n",
    "data.dropna(axis = 1, inplace = True)\n",
    "data = data.drop(columns = [\n",
    "    'rad_name', 'h_name', 'hash_u1', 'hash_u2', 'e_max_key', 'e_max', 'e_00', 'e_03', 'e_04',\n",
    "    'e_05', 'e_06', 'e_07', 'e_10', 'meta_path', 'dG', 'rad_name_s',\n",
    "    'h_name_s', 'reaction', 'trans_mean', 'Ea_trans_mean', 'rad_ref',\n",
    "    'rad_ref_idx', 'h_ref', 'h_ref_idx', 'bur_vol_default_rad', 'bur_vol_2A_rad',\n",
    "    'bur_vol_default_H', 'bur_vol_2A_H'\n",
    "], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute enthalpies of reaction to use as a direct input\n",
    "BDE_rad = data['rad_BDE']\n",
    "BDE_H = data['H_BDE']\n",
    "Delta_H = BDE_H - BDE_rad\n",
    "Delta_H_df = pd.DataFrame(Delta_H, columns=['Delta_H'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_column(dataframe_in, column):\n",
    "    dataframe = dataframe_in.reset_index(drop=True)\n",
    "\n",
    "    if len(dataframe[column][0].shape) > 1:\n",
    "        max_length = max([len(row[0]) for row in dataframe[column]])\n",
    "    else:\n",
    "        max_length = max([len(row) for row in dataframe[column]])\n",
    "\n",
    "    unpacked = np.empty((len(dataframe[column]), max_length))\n",
    "\n",
    "    for i, row in zip(range(len(dataframe[column])), dataframe[column]):\n",
    "        if len(row.shape) > 1:\n",
    "            row = row[0]\n",
    "\n",
    "        length_diff = max_length-len(row)\n",
    "        if length_diff > 0:\n",
    "            diff = np.zeros(max_length-len(row))\n",
    "            expanded = np.concatenate((row, diff), axis=1)\n",
    "            unpacked[i] = expanded\n",
    "        else:\n",
    "            unpacked[i] = row\n",
    "\n",
    "    return pd.DataFrame(unpacked)\n",
    "\n",
    "def drop_zeros(dataframe):\n",
    "    df = dataframe.loc[:, (dataframe != 0).any(axis = 0)]\n",
    "    return df\n",
    "    \n",
    "def unpack_join(dataframe, columns):\n",
    "    unpacked_dfs = []\n",
    "    for column in columns:\n",
    "        unpacked_dfs.append(unpack_column(dataframe, column))\n",
    "\n",
    "    dataframe_compl = dataframe.reset_index(drop=True)\n",
    "    dataframe_compl.drop(columns = columns, inplace=True)\n",
    "    for i, df in zip(range(len(unpacked_dfs)), unpacked_dfs):\n",
    "        dataframe_compl = dataframe_compl.join(df, lsuffix = '_{}'.format(i-1) , rsuffix = '_{}'.format(i))\n",
    "    \n",
    "    return dataframe_compl  \n",
    "\n",
    "def plot_loss(history, no_epochs, title):\n",
    "    fs = 18\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    ax.plot(history.history['loss'], label='Loss')\n",
    "    ax.plot(history.history['val_loss'], label='Validation loss')\n",
    "    ax.set_xlim([0, no_epochs])\n",
    "    ax.set_xlabel('Epoch', fontsize = fs)\n",
    "    ax.set_ylabel('Error [kcal/mol]', fontsize = fs)\n",
    "    ax.tick_params(labelsize = fs)\n",
    "    plt.legend(fontsize = fs)\n",
    "    plt.grid(True)\n",
    "    plt.title(title, fontsize = fs)\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions(model, test_features, test_labels, title):\n",
    "    fs = 18\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    test_predictions = model.predict(test_features).flatten()\n",
    "    ax.scatter(test_predictions, test_labels)\n",
    "    ax.set_xlabel('Predicted Barrier [kcal/mol]', fontsize = fs)\n",
    "    ax.set_ylabel('True Barrier [kcal/mol]', fontsize = fs)\n",
    "    lims = [0, max(test_labels)+10]\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "    ax.plot(lims, lims)\n",
    "    ax.tick_params(labelsize = fs)\n",
    "    plt.title(title, fontsize = fs)\n",
    "    plt.show()\n",
    "\n",
    "def plot_errors(model, test_features, test_labels, title):\n",
    "    fs = 18\n",
    "    fig, ax = plt.subplots(figsize = (10,10))\n",
    "    test_predictions = model.predict(test_features).flatten()\n",
    "    error = test_predictions - test_labels\n",
    "    ax.hist(error, bins=5)\n",
    "    ax.set_xlabel('Prediction error [kcal/mol]', fontsize = fs)\n",
    "    ax.set_ylabel('Count', fontsize = fs)\n",
    "    lims = [0, max(test_labels)+10]\n",
    "    ax.set_xlim(lims)\n",
    "    ax.set_ylim(lims)\n",
    "    ax.plot(lims, lims)\n",
    "    ax.tick_params(labelsize = fs)\n",
    "    plt.title(title, fontsize = fs)\n",
    "    plt.show()\n",
    "\n",
    "def K_fold_cross_validation(\n",
    "  inputs, targets, num_folds, model, mixed_dtypes, loss='MSLE', metrics = ['mae'], batch_size = 100, no_epochs = 500, verbose = 0,\n",
    "  split = 0.2, callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "):\n",
    "    mae_per_fold = []\n",
    "    loss_per_fold = []\n",
    "  \n",
    "    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "        0.001, decay_steps=inputs.shape[0]*0.8/batch_size*1000, decay_rate=1, staircase=False\n",
    "    )\n",
    "    optimizer=tf.keras.optimizers.Adam(lr_schedule)\n",
    "  \n",
    "  \n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state = 1)\n",
    "    \n",
    "    # K-fold Cross Validation model evaluation\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "  \n",
    "        model.compile(loss=loss,\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=metrics)\n",
    "                    \n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "        if mixed_dtypes:\n",
    "            # inputs will be a DataFrame\n",
    "            input_train_dict = {}\n",
    "            input_train_dict = {name: np.array(value) \n",
    "                         for name, value in inputs.iloc[train,:].items()}\n",
    "    \n",
    "            history = model.fit(input_train_dict, targets[train],\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=no_epochs,\n",
    "                        verbose=verbose,\n",
    "                        validation_split=split,\n",
    "                        callbacks=callbacks)\n",
    "\n",
    "            input_test_dict = {}\n",
    "            input_test_dict = {name: np.array(value) \n",
    "                         for name, value in inputs.iloc[test,:].items()}\n",
    "\n",
    "            scores = model.evaluate(input_test_dict, targets[test], verbose=0)\n",
    "\n",
    "        else:\n",
    "            # inputs will be a np.array\n",
    "            history = model.fit(inputs[train], targets[train],\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=no_epochs,\n",
    "                        verbose=verbose,\n",
    "                        validation_split=split,\n",
    "                        callbacks=callbacks)\n",
    "            scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        \n",
    "        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]}')\n",
    "        mae_per_fold.append(scores[1])\n",
    "        loss_per_fold.append(scores[0])\n",
    "      \n",
    "        fold_no = fold_no + 1\n",
    "    \n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Score per fold')\n",
    "    for i in range(0, len(mae_per_fold)):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - MAE: {mae_per_fold[i]}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Average scores for all folds:')\n",
    "    print(f'> MAE: {np.mean(mae_per_fold)} (+- {np.std(mae_per_fold)})')\n",
    "    print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "    print('------------------------------------------------------------------------')\n",
    "  \n",
    "    return mae_per_fold, loss_per_fold\n",
    "\n",
    "def plot_cross_val_scores(dataframe):\n",
    "    sns.set_style('ticks')\n",
    "    ax = sns.violinplot(x=\"Model\", y=\"MAE [kcal/mol]\", data=dataframe, inner=None, orient='v')\n",
    "    ax = sns.swarmplot(x=\"Model\", y=\"MAE [kcal/mol]\", data=dataframe, color='black', edgecolor='black')\n",
    "    # sns.set(font_scale=1.1)\n",
    "\n",
    "def plot_nn(names, shapes, activations, file_name, title):\n",
    "    lengths = [len(names), len(shapes), len(activations)]\n",
    "    lengths_comp = [len(names) for i in range(3)]\n",
    "    assert np.allclose(lengths, lengths_comp), 'All input lists must be of the same length.'\n",
    "\n",
    "    dot = Digraph(\n",
    "        node_attr={'shape': 'box', 'fillcolor': 'lightblue2', 'style': 'filled'},\n",
    "    )\n",
    "    dot.attr(label=title)\n",
    "    for i, n, s, a in zip(range(len(names)), names, shapes, activations):\n",
    "        if s != None:\n",
    "            dot.node('L{}'.format(i), '{}\\n{} nodes\\nActivation: {}'.format(n, s, a))\n",
    "        else:\n",
    "            dot.node('L{}'.format(i), '{}'.format(n))\n",
    "\n",
    "    for i, j in zip(range(len(names)-1), range(1,len(names))):\n",
    "        dot.edge('L{}'.format(i), 'L{}'.format(j))\n",
    "        \n",
    "    dot.format = 'png'\n",
    "    dot.render(file_name)\n",
    "\n",
    "def build_standard_model(input_shape):\n",
    "    input = tf.keras.layers.Input(shape=(input_shape), dtype='float32')\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(input)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(64, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(1)(out)\n",
    "    model = tf.keras.Model(input, out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_soap_se_nomordred = data[[\n",
    "    'Ea', 'translation', 'rad_charge', 'h_charge', 'rad_BDE', 'H_BDE',\n",
    "    'max_spin_rad', 'mull_charge_rad', 'bur_vol_iso_rad',\n",
    "    'max_spin_H', 'mull_charge_H', 'bur_vol_iso_H',\n",
    "    'soap_H_start', 'soap_H_end'\n",
    "]]\n",
    "\n",
    "data_soap_se_noBDEs = data[[\n",
    "    'Ea', 'translation', 'rad_charge', 'h_charge',\n",
    "    'max_spin_rad', 'mull_charge_rad', 'bur_vol_iso_rad',\n",
    "    'max_spin_H', 'mull_charge_H', 'bur_vol_iso_H',\n",
    "    'nBase_rad', 'SpMax_A_rad', 'ATSC2s_rad', 'ATSC1Z_rad', 'ATSC2i_rad',\n",
    "    'NdNH_rad', 'SMR_VSA4_rad', 'nBase_H', 'SpMax_A_H', 'ATSC2s_H',\n",
    "    'ATSC1Z_H', 'ATSC2i_H', 'GATS2dv_H', 'BCUTdv-1h_H', 'SMR_VSA4_H',\n",
    "    'VSA_EState7_H', 'soap_H_start', 'soap_H_end'\n",
    "]]\n",
    "\n",
    "data_soap_DeltaH = data[[\n",
    "    'Ea', 'translation', 'rad_charge', 'h_charge',\n",
    "    'max_spin_rad', 'mull_charge_rad', 'bur_vol_iso_rad',\n",
    "    'max_spin_H', 'mull_charge_H', 'bur_vol_iso_H',\n",
    "    'nBase_rad', 'SpMax_A_rad', 'ATSC2s_rad', 'ATSC1Z_rad', 'ATSC2i_rad',\n",
    "    'NdNH_rad', 'SMR_VSA4_rad', 'nBase_H', 'SpMax_A_H', 'ATSC2s_H',\n",
    "    'ATSC1Z_H', 'ATSC2i_H', 'GATS2dv_H', 'BCUTdv-1h_H', 'SMR_VSA4_H',\n",
    "    'VSA_EState7_H', 'soap_H_start', 'soap_H_end'\n",
    "]]\n",
    "data_soap_DeltaH = data_soap_DeltaH.join(Delta_H_df)\n",
    "\n",
    "data_lmbtr_se_noBDEs = data[[\n",
    "    'Ea', 'translation', 'rad_charge', 'h_charge',\n",
    "    'max_spin_rad', 'mull_charge_rad', 'bur_vol_iso_rad',\n",
    "    'max_spin_H', 'mull_charge_H', 'bur_vol_iso_H',\n",
    "    'nBase_rad', 'SpMax_A_rad', 'ATSC2s_rad', 'ATSC1Z_rad', 'ATSC2i_rad',\n",
    "    'NdNH_rad', 'SMR_VSA4_rad', 'nBase_H', 'SpMax_A_H', 'ATSC2s_H',\n",
    "    'ATSC1Z_H', 'ATSC2i_H', 'GATS2dv_H', 'BCUTdv-1h_H', 'SMR_VSA4_H',\n",
    "    'VSA_EState7_H', 'lmbtr_H_start', 'lmbtr_H_end'\n",
    "]]\n",
    "\n",
    "data_lmbtr_DeltaH = data[[\n",
    "    'Ea', 'translation', 'rad_charge', 'h_charge',\n",
    "    'max_spin_rad', 'mull_charge_rad', 'bur_vol_iso_rad',\n",
    "    'max_spin_H', 'mull_charge_H', 'bur_vol_iso_H',\n",
    "    'nBase_rad', 'SpMax_A_rad', 'ATSC2s_rad', 'ATSC1Z_rad', 'ATSC2i_rad',\n",
    "    'NdNH_rad', 'SMR_VSA4_rad', 'nBase_H', 'SpMax_A_H', 'ATSC2s_H',\n",
    "    'ATSC1Z_H', 'ATSC2i_H', 'GATS2dv_H', 'BCUTdv-1h_H', 'SMR_VSA4_H',\n",
    "    'VSA_EState7_H', 'lmbtr_H_start', 'lmbtr_H_end'\n",
    "]]\n",
    "data_lmbtr_DeltaH = data_lmbtr_DeltaH.join(Delta_H_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_soap_se_nomordred = unpack_join(data_soap_se_nomordred, ['soap_H_start', 'soap_H_end'])\n",
    "data_soap_se_nomordred = drop_zeros(data_soap_se_nomordred)\n",
    "\n",
    "data_soap_se_noBDEs = unpack_join(data_soap_se_noBDEs, ['soap_H_start', 'soap_H_end'])\n",
    "data_soap_se_noBDEs = drop_zeros(data_soap_se_noBDEs)\n",
    "\n",
    "data_soap_DeltaH = unpack_join(data_soap_DeltaH, ['soap_H_start', 'soap_H_end'])\n",
    "data_soap_DeltaH = drop_zeros(data_soap_DeltaH)\n",
    "\n",
    "data_lmbtr_se_noBDEs = unpack_join(data_lmbtr_se_noBDEs, ['lmbtr_H_start', 'lmbtr_H_end'])\n",
    "data_lmbtr_se_noBDEs = drop_zeros(data_lmbtr_se_noBDEs)\n",
    "\n",
    "data_lmbtr_DeltaH = unpack_join(data_lmbtr_DeltaH, ['lmbtr_H_start', 'lmbtr_H_end'])\n",
    "data_lmbtr_DeltaH = drop_zeros(data_lmbtr_DeltaH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Shape of inputs:\n",
      "\n",
      "    data_soap_se_nomordred: 11492\n",
      "\n",
      "    data_soap_se_noBDEs: 11506\n",
      "\n",
      "    data_soap_DeltaH: 11507\n",
      "\n",
      "    data_lmbtr_se_noBDEs: 8240\n",
      "\n",
      "    data_lmbtr_DeltaH: 8241\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    '''\n",
    "    Shape of inputs:\\n\n",
    "    data_soap_se_nomordred: {}\\n\n",
    "    data_soap_se_noBDEs: {}\\n\n",
    "    data_soap_DeltaH: {}\\n\n",
    "    data_lmbtr_se_noBDEs: {}\\n\n",
    "    data_lmbtr_DeltaH: {}\n",
    "    '''.format(\n",
    "        data_soap_se_nomordred.shape[1],\n",
    "        data_soap_se_noBDEs.shape[1],\n",
    "        data_soap_DeltaH.shape[1],\n",
    "        data_lmbtr_se_noBDEs.shape[1],\n",
    "        data_lmbtr_DeltaH.shape[1]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduce Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.013965606689453; mae of 8.013965606689453\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.976191997528076; mae of 5.976191997528076\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.625258922576904; mae of 5.625258922576904\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.338718891143799; mae of 4.338718891143799\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.963077545166016; mae of 4.963077545166016\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.5812811851501465; mae of 4.5812811851501465\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 3.925279378890991; mae of 3.925279378890991\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.125584125518799; mae of 4.125584125518799\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.019586086273193; mae of 4.019586086273193\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.23252534866333; mae of 4.23252534866333\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.013965606689453 - MAE: 8.013965606689453\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.976191997528076 - MAE: 5.976191997528076\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.625258922576904 - MAE: 5.625258922576904\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.338718891143799 - MAE: 4.338718891143799\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.963077545166016 - MAE: 4.963077545166016\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.5812811851501465 - MAE: 4.5812811851501465\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.925279378890991 - MAE: 3.925279378890991\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.125584125518799 - MAE: 4.125584125518799\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.019586086273193 - MAE: 4.019586086273193\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.23252534866333 - MAE: 4.23252534866333\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.980146908760071 (+- 1.2044826482126854)\n",
      "> Loss: 4.980146908760071\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors\n",
    "targets = data_soap_se_nomordred.pop('Ea')\n",
    "inputs = np.array(data_soap_se_nomordred)\n",
    "\n",
    "normalize = tf.keras.layers.Normalization()\n",
    "normalize.adapt(inputs)\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(11491), dtype='float32')\n",
    "out = normalize(input)\n",
    "out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.BatchNormalization()(out)\n",
    "out = tf.keras.layers.Dense(64, activation='relu')(out)\n",
    "out = tf.keras.layers.BatchNormalization()(out)\n",
    "out = tf.keras.layers.Dense(1)(out)\n",
    "model = tf.keras.Model(input, out)\n",
    "\n",
    "mae_per_fold_soap_se_nomordred, loss_per_fold_soap_se_nomordred = K_fold_cross_validation(\n",
    "    inputs, targets, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.202195167541504; mae of 8.202195167541504\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.427987575531006; mae of 6.427987575531006\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.738282203674316; mae of 5.738282203674316\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.562415599822998; mae of 4.562415599822998\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 5.016743183135986; mae of 5.016743183135986\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.356122016906738; mae of 4.356122016906738\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.594699382781982; mae of 4.594699382781982\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.209686756134033; mae of 4.209686756134033\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.251641273498535; mae of 4.251641273498535\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.514966011047363; mae of 4.514966011047363\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.202195167541504 - MAE: 8.202195167541504\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.427987575531006 - MAE: 6.427987575531006\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.738282203674316 - MAE: 5.738282203674316\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.562415599822998 - MAE: 4.562415599822998\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 5.016743183135986 - MAE: 5.016743183135986\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.356122016906738 - MAE: 4.356122016906738\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.594699382781982 - MAE: 4.594699382781982\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.209686756134033 - MAE: 4.209686756134033\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.251641273498535 - MAE: 4.251641273498535\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.514966011047363 - MAE: 4.514966011047363\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.187473917007447 (+- 1.211499070980561)\n",
      "> Loss: 5.187473917007447\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, no BDEs\n",
    "targets = data_soap_se_noBDEs.pop('Ea')\n",
    "inputs = np.array(data_soap_se_noBDEs)\n",
    "\n",
    "normalize = tf.keras.layers.Normalization()\n",
    "normalize.adapt(inputs)\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(11505), dtype='float32')\n",
    "out = normalize(input)\n",
    "out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.BatchNormalization()(out)\n",
    "out = tf.keras.layers.Dense(64, activation='relu')(out)\n",
    "out = tf.keras.layers.BatchNormalization()(out)\n",
    "out = tf.keras.layers.Dense(1)(out)\n",
    "model = tf.keras.Model(input, out)\n",
    "\n",
    "mae_per_fold_soap_se_noBDEs, loss_per_fold_soap_se_noBDEs = K_fold_cross_validation(\n",
    "    inputs, targets, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.333030700683594; mae of 8.333030700683594\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.098076820373535; mae of 6.098076820373535\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.539495944976807; mae of 5.539495944976807\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.559484481811523; mae of 4.559484481811523\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.750124454498291; mae of 4.750124454498291\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.2207231521606445; mae of 4.2207231521606445\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.088287830352783; mae of 4.088287830352783\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.397552967071533; mae of 4.397552967071533\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.028057098388672; mae of 4.028057098388672\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.337663173675537; mae of 4.337663173675537\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.333030700683594 - MAE: 8.333030700683594\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.098076820373535 - MAE: 6.098076820373535\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.539495944976807 - MAE: 5.539495944976807\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.559484481811523 - MAE: 4.559484481811523\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.750124454498291 - MAE: 4.750124454498291\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.2207231521606445 - MAE: 4.2207231521606445\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.088287830352783 - MAE: 4.088287830352783\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.397552967071533 - MAE: 4.397552967071533\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.028057098388672 - MAE: 4.028057098388672\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.337663173675537 - MAE: 4.337663173675537\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.035249662399292 (+- 1.26628533678801)\n",
      "> Loss: 5.035249662399292\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors, enthalpy of reaction\n",
    "targets = data_soap_DeltaH.pop('Ea')\n",
    "inputs = np.array(data_soap_DeltaH)\n",
    "\n",
    "normalize = tf.keras.layers.Normalization()\n",
    "normalize.adapt(inputs)\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(11506), dtype='float32')\n",
    "out = normalize(input)\n",
    "out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.BatchNormalization()(out)\n",
    "out = tf.keras.layers.Dense(64, activation='relu')(out)\n",
    "out = tf.keras.layers.BatchNormalization()(out)\n",
    "out = tf.keras.layers.Dense(1)(out)\n",
    "model = tf.keras.Model(input, out)\n",
    "\n",
    "mae_per_fold_soap_DeltaH, loss_per_fold_soap_DeltaH = K_fold_cross_validation(\n",
    "    inputs, targets, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.266844749450684; mae of 8.266844749450684\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.354099750518799; mae of 5.354099750518799\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 4.664579391479492; mae of 4.664579391479492\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 3.975088596343994; mae of 3.975088596343994\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.582648277282715; mae of 4.582648277282715\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 3.981537342071533; mae of 3.981537342071533\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.059120178222656; mae of 4.059120178222656\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 3.732922077178955; mae of 3.732922077178955\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.026821136474609; mae of 4.026821136474609\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 3.9403443336486816; mae of 3.9403443336486816\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.266844749450684 - MAE: 8.266844749450684\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.354099750518799 - MAE: 5.354099750518799\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 4.664579391479492 - MAE: 4.664579391479492\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 3.975088596343994 - MAE: 3.975088596343994\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.582648277282715 - MAE: 4.582648277282715\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 3.981537342071533 - MAE: 3.981537342071533\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.059120178222656 - MAE: 4.059120178222656\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 3.732922077178955 - MAE: 3.732922077178955\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.026821136474609 - MAE: 4.026821136474609\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 3.9403443336486816 - MAE: 3.9403443336486816\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.658400583267212 (+- 1.2870838039690595)\n",
      "> Loss: 4.658400583267212\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LMBTR descriptors, Mordred descriptors\n",
    "targets = data_lmbtr_se_noBDEs.pop('Ea')\n",
    "inputs = np.array(data_lmbtr_se_noBDEs)\n",
    "\n",
    "normalize = tf.keras.layers.Normalization()\n",
    "normalize.adapt(inputs)\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(8239), dtype='float32')\n",
    "out = normalize(input)\n",
    "out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.BatchNormalization()(out)\n",
    "out = tf.keras.layers.Dense(64, activation='relu')(out)\n",
    "out = tf.keras.layers.BatchNormalization()(out)\n",
    "out = tf.keras.layers.Dense(1)(out)\n",
    "model = tf.keras.Model(input, out)\n",
    "\n",
    "mae_per_fold_lmbtr_se_noBDEs, loss_per_fold_lmbtr_se_noBDEs = K_fold_cross_validation(\n",
    "    inputs, targets, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.305031776428223; mae of 8.305031776428223\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.074409008026123; mae of 5.074409008026123\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 4.699100971221924; mae of 4.699100971221924\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.084972858428955; mae of 4.084972858428955\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.359579563140869; mae of 4.359579563140869\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.083934307098389; mae of 4.083934307098389\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 3.8337290287017822; mae of 3.8337290287017822\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 3.8998875617980957; mae of 3.8998875617980957\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.71683669090271; mae of 3.71683669090271\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 3.766651153564453; mae of 3.766651153564453\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.305031776428223 - MAE: 8.305031776428223\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.074409008026123 - MAE: 5.074409008026123\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 4.699100971221924 - MAE: 4.699100971221924\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.084972858428955 - MAE: 4.084972858428955\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.359579563140869 - MAE: 4.359579563140869\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.083934307098389 - MAE: 4.083934307098389\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.8337290287017822 - MAE: 3.8337290287017822\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 3.8998875617980957 - MAE: 3.8998875617980957\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.71683669090271 - MAE: 3.71683669090271\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 3.766651153564453 - MAE: 3.766651153564453\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.582413291931152 (+- 1.307677618952936)\n",
      "> Loss: 4.582413291931152\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LMBTR descriptors, Mordred descriptors, enthalpy of reaction\n",
    "targets = data_lmbtr_DeltaH.pop('Ea')\n",
    "inputs = np.array(data_lmbtr_DeltaH)\n",
    "\n",
    "normalize = tf.keras.layers.Normalization()\n",
    "normalize.adapt(inputs)\n",
    "\n",
    "input = tf.keras.layers.Input(shape=(8240), dtype='float32')\n",
    "out = normalize(input)\n",
    "out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.BatchNormalization()(out)\n",
    "out = tf.keras.layers.Dense(64, activation='relu')(out)\n",
    "out = tf.keras.layers.BatchNormalization()(out)\n",
    "out = tf.keras.layers.Dense(1)(out)\n",
    "model = tf.keras.Model(input, out)\n",
    "\n",
    "mae_per_fold_lmbtr_DeltaH, loss_per_fold_lmbtr_DeltaH = K_fold_cross_validation(\n",
    "    inputs, targets, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyper_opt_norm = pd.DataFrame({'Model': [\n",
    "    *['Model 1' for i in range(len(mae_per_fold_soap_se_nomordred))],\n",
    "    *['Model 2' for i in range(len(mae_per_fold_soap_se_noBDEs))],\n",
    "    *['Model 3' for i in range(len(mae_per_fold_soap_DeltaH))],\n",
    "    *['Model 4' for i in range(len(mae_per_fold_lmbtr_se_noBDEs))],\n",
    "    *['Model 5' for i in range(len(mae_per_fold_lmbtr_DeltaH))]\n",
    "], 'MAE [kcal/mol]': [\n",
    "    *mae_per_fold_soap_se_nomordred, *mae_per_fold_soap_se_noBDEs, *mae_per_fold_soap_DeltaH,\n",
    "    *mae_per_fold_lmbtr_se_noBDEs, *mae_per_fold_lmbtr_DeltaH\n",
    "]})\n",
    "\n",
    "df_hyper_opt_norm.to_csv('cross_val_scores_hyper_opt_norm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of normalization above is incompatible with the tensorflow version required for GPU acceleration, so for further hyperparameter optimization we'll normalize the date before we pass it to the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the data first\n",
    "data_soap_se_nomordred_target = data_soap_se_nomordred.pop('Ea')\n",
    "data_soap_se_noBDEs_target = data_soap_se_noBDEs.pop('Ea')\n",
    "data_soap_DeltaH_target = data_soap_DeltaH.pop('Ea')\n",
    "data_lmbtr_se_noBDEs_target = data_lmbtr_se_noBDEs.pop('Ea')\n",
    "data_lmbtr_DeltaH_target = data_lmbtr_DeltaH.pop('Ea')\n",
    "\n",
    "data_soap_se_nomordred = pd.DataFrame(StandardScaler().fit_transform(data_soap_se_nomordred), columns = list(data_soap_se_nomordred.columns))\n",
    "data_soap_se_noBDEs = pd.DataFrame(StandardScaler().fit_transform(data_soap_se_noBDEs), columns = list(data_soap_se_noBDEs.columns))\n",
    "data_soap_DeltaH = pd.DataFrame(StandardScaler().fit_transform(data_soap_DeltaH), columns = list(data_soap_DeltaH.columns))\n",
    "data_lmbtr_se_noBDEs = pd.DataFrame(StandardScaler().fit_transform(data_lmbtr_se_noBDEs), columns = list(data_lmbtr_se_noBDEs.columns))\n",
    "data_lmbtr_DeltaH = pd.DataFrame(StandardScaler().fit_transform(data_lmbtr_DeltaH), columns = list(data_lmbtr_DeltaH.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add layers (up to 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_2(input_shape):\n",
    "    input = tf.keras.layers.Input(shape=(input_shape), dtype='float32')\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(input)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(64, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(1)(out)\n",
    "    model = tf.keras.Model(input, out)\n",
    "    return model\n",
    "\n",
    "def build_model_3(input_shape):\n",
    "    input = tf.keras.layers.Input(shape=(input_shape), dtype='float32')\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(input)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(64, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(1)(out)\n",
    "    model = tf.keras.Model(input, out)\n",
    "    return model\n",
    "\n",
    "def build_model_4(input_shape):\n",
    "    input = tf.keras.layers.Input(shape=(input_shape), dtype='float32')\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(input)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(64, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(1)(out)\n",
    "    model = tf.keras.Model(input, out)\n",
    "    return model\n",
    "\n",
    "def build_model_5(input_shape):\n",
    "    input = tf.keras.layers.Input(shape=(input_shape), dtype='float32')\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(input)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(64, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(1)(out)\n",
    "    model = tf.keras.Model(input, out)\n",
    "    return model\n",
    "\n",
    "def build_model_6(input_shape):\n",
    "    input = tf.keras.layers.Input(shape=(input_shape), dtype='float32')\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(input)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(128, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(64, activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(1)(out)\n",
    "    model = tf.keras.Model(input, out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.090597152709961; mae of 8.090597152709961\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.224880218505859; mae of 6.224880218505859\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.649747371673584; mae of 5.649747371673584\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.367082118988037; mae of 4.367082118988037\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.847179889678955; mae of 4.847179889678955\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.263080596923828; mae of 4.263080596923828\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.052885055541992; mae of 4.052885055541992\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.302666187286377; mae of 4.302666187286377\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.152908802032471; mae of 4.152908802032471\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.167910099029541; mae of 4.167910099029541\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.090597152709961 - MAE: 8.090597152709961\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.224880218505859 - MAE: 6.224880218505859\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.649747371673584 - MAE: 5.649747371673584\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.367082118988037 - MAE: 4.367082118988037\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.847179889678955 - MAE: 4.847179889678955\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.263080596923828 - MAE: 4.263080596923828\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.052885055541992 - MAE: 4.052885055541992\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.302666187286377 - MAE: 4.302666187286377\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.152908802032471 - MAE: 4.152908802032471\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.167910099029541 - MAE: 4.167910099029541\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.0118937492370605 (+- 1.2342247302054568)\n",
      "> Loss: 5.0118937492370605\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors\n",
    "model = build_model_2(11491)\n",
    "inputs = np.array(data_soap_se_nomordred)\n",
    "mae_per_fold_soap_se_nomordred_2, loss_per_fold_soap_se_nomordred_2 = K_fold_cross_validation(\n",
    "    inputs, data_soap_se_nomordred_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.046910285949707; mae of 8.046910285949707\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.037797927856445; mae of 6.037797927856445\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.384449481964111; mae of 5.384449481964111\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.563538074493408; mae of 4.563538074493408\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.916438102722168; mae of 4.916438102722168\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.1573405265808105; mae of 4.1573405265808105\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.102823734283447; mae of 4.102823734283447\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.178613185882568; mae of 4.178613185882568\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.137556076049805; mae of 4.137556076049805\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.112007141113281; mae of 4.112007141113281\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.046910285949707 - MAE: 8.046910285949707\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.037797927856445 - MAE: 6.037797927856445\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.384449481964111 - MAE: 5.384449481964111\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.563538074493408 - MAE: 4.563538074493408\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.916438102722168 - MAE: 4.916438102722168\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.1573405265808105 - MAE: 4.1573405265808105\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.102823734283447 - MAE: 4.102823734283447\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.178613185882568 - MAE: 4.178613185882568\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.137556076049805 - MAE: 4.137556076049805\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.112007141113281 - MAE: 4.112007141113281\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.963747453689575 (+- 1.200618371654042)\n",
      "> Loss: 4.963747453689575\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, BDEs\n",
    "model = build_model_2(11505)\n",
    "inputs = np.array(data_soap_se_noBDEs)\n",
    "mae_per_fold_soap_se_noBDEs_2, loss_per_fold_soap_se_noBDEs_2 = K_fold_cross_validation(\n",
    "    inputs, data_soap_se_noBDEs_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.091449737548828; mae of 8.091449737548828\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.927804470062256; mae of 5.927804470062256\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.8144917488098145; mae of 5.8144917488098145\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.698416709899902; mae of 4.698416709899902\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.906053066253662; mae of 4.906053066253662\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.4060378074646; mae of 4.4060378074646\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.095154285430908; mae of 4.095154285430908\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.410027027130127; mae of 4.410027027130127\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.053435802459717; mae of 4.053435802459717\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.3632707595825195; mae of 4.3632707595825195\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.091449737548828 - MAE: 8.091449737548828\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.927804470062256 - MAE: 5.927804470062256\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.8144917488098145 - MAE: 5.8144917488098145\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.698416709899902 - MAE: 4.698416709899902\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.906053066253662 - MAE: 4.906053066253662\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.4060378074646 - MAE: 4.4060378074646\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.095154285430908 - MAE: 4.095154285430908\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.410027027130127 - MAE: 4.410027027130127\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.053435802459717 - MAE: 4.053435802459717\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.3632707595825195 - MAE: 4.3632707595825195\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.076614141464233 (+- 1.18082062700321)\n",
      "> Loss: 5.076614141464233\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors, enthalpy of reaction\n",
    "model = build_model_2(11506)\n",
    "inputs = np.array(data_soap_DeltaH)\n",
    "mae_per_fold_soap_DeltaH_2, loss_per_fold_soap_DeltaH_2 = K_fold_cross_validation(\n",
    "    inputs, data_soap_DeltaH_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.181506156921387; mae of 8.181506156921387\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.480401039123535; mae of 5.480401039123535\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 4.351189136505127; mae of 4.351189136505127\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 3.9752748012542725; mae of 3.9752748012542725\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.494346618652344; mae of 4.494346618652344\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.374698162078857; mae of 4.374698162078857\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 3.713958978652954; mae of 3.713958978652954\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 3.6102919578552246; mae of 3.6102919578552246\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.989473342895508; mae of 3.989473342895508\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.150766372680664; mae of 4.150766372680664\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.181506156921387 - MAE: 8.181506156921387\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.480401039123535 - MAE: 5.480401039123535\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 4.351189136505127 - MAE: 4.351189136505127\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 3.9752748012542725 - MAE: 3.9752748012542725\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.494346618652344 - MAE: 4.494346618652344\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.374698162078857 - MAE: 4.374698162078857\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.713958978652954 - MAE: 3.713958978652954\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 3.6102919578552246 - MAE: 3.6102919578552246\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.989473342895508 - MAE: 3.989473342895508\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.150766372680664 - MAE: 4.150766372680664\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.6321906566619875 (+- 1.2823878619216391)\n",
      "> Loss: 4.6321906566619875\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LMBTR descriptors, Mordred descriptors\n",
    "model = build_model_2(8239)\n",
    "inputs = np.array(data_lmbtr_se_noBDEs)\n",
    "mae_per_fold_lmbtr_se_noBDEs_2, loss_per_fold_lmbtr_se_noBDEs_2 = K_fold_cross_validation(\n",
    "    inputs, data_lmbtr_se_noBDEs_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.074642181396484; mae of 8.074642181396484\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.372731685638428; mae of 5.372731685638428\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 4.708434581756592; mae of 4.708434581756592\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 3.8841285705566406; mae of 3.8841285705566406\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.156886577606201; mae of 4.156886577606201\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.141538619995117; mae of 4.141538619995117\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 3.933649778366089; mae of 3.933649778366089\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 3.6678476333618164; mae of 3.6678476333618164\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.764225959777832; mae of 3.764225959777832\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.011416435241699; mae of 4.011416435241699\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.074642181396484 - MAE: 8.074642181396484\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.372731685638428 - MAE: 5.372731685638428\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 4.708434581756592 - MAE: 4.708434581756592\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 3.8841285705566406 - MAE: 3.8841285705566406\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.156886577606201 - MAE: 4.156886577606201\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.141538619995117 - MAE: 4.141538619995117\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.933649778366089 - MAE: 3.933649778366089\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 3.6678476333618164 - MAE: 3.6678476333618164\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.764225959777832 - MAE: 3.764225959777832\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.011416435241699 - MAE: 4.011416435241699\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.57155020236969 (+- 1.2629649422080729)\n",
      "> Loss: 4.57155020236969\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LMBTR descriptors, Mordred descriptors, enthalpy of reaction\n",
    "model = build_model_2(8240)\n",
    "inputs = np.array(data_lmbtr_DeltaH)\n",
    "mae_per_fold_lmbtr_DeltaH_2, loss_per_fold_lmbtr_DeltaH_2 = K_fold_cross_validation(\n",
    "    inputs, data_lmbtr_DeltaH_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyper_opt_more_layers = pd.DataFrame({'Model': [\n",
    "    *['Model 1 (4 layers)' for i in range(len(mae_per_fold_soap_se_nomordred_2))],\n",
    "    *['Model 2 (4 layers)' for i in range(len(mae_per_fold_soap_se_noBDEs_2))],\n",
    "    *['Model 3 (4 layers)' for i in range(len(mae_per_fold_soap_DeltaH_2))],\n",
    "    *['Model 4 (4 layers)' for i in range(len(mae_per_fold_lmbtr_se_noBDEs_2))],\n",
    "    *['Model 5 (4 layers)' for i in range(len(mae_per_fold_lmbtr_DeltaH_2))]\n",
    "], 'MAE [kcal/mol]': [\n",
    "    *mae_per_fold_soap_se_nomordred_2, *mae_per_fold_soap_se_noBDEs_2, *mae_per_fold_soap_DeltaH_2,\n",
    "    *mae_per_fold_lmbtr_se_noBDEs_2, *mae_per_fold_lmbtr_DeltaH_2\n",
    "]})\n",
    "\n",
    "df_hyper_opt_more_layers.to_csv('cross_val_scores_hyper_opt_more_layers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 7.725742340087891; mae of 7.725742340087891\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.308877468109131; mae of 6.308877468109131\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.510425567626953; mae of 5.510425567626953\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.984089374542236; mae of 4.984089374542236\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.95871639251709; mae of 4.95871639251709\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.3913140296936035; mae of 4.3913140296936035\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.232489109039307; mae of 4.232489109039307\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.086076259613037; mae of 4.086076259613037\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.243788242340088; mae of 4.243788242340088\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.497720241546631; mae of 4.497720241546631\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 7.725742340087891 - MAE: 7.725742340087891\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.308877468109131 - MAE: 6.308877468109131\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.510425567626953 - MAE: 5.510425567626953\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.984089374542236 - MAE: 4.984089374542236\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.95871639251709 - MAE: 4.95871639251709\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.3913140296936035 - MAE: 4.3913140296936035\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.232489109039307 - MAE: 4.232489109039307\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.086076259613037 - MAE: 4.086076259613037\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.243788242340088 - MAE: 4.243788242340088\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.497720241546631 - MAE: 4.497720241546631\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.093923902511596 (+- 1.092523484433499)\n",
      "> Loss: 5.093923902511596\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors\n",
    "model = build_model_3(11491)\n",
    "inputs = np.array(data_soap_se_nomordred)\n",
    "mae_per_fold_soap_se_nomordred_3, loss_per_fold_soap_se_nomordred_3 = K_fold_cross_validation(\n",
    "    inputs, data_soap_se_nomordred_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 7.769536018371582; mae of 7.769536018371582\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.313778877258301; mae of 6.313778877258301\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.504204273223877; mae of 5.504204273223877\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.686505317687988; mae of 4.686505317687988\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.909460544586182; mae of 4.909460544586182\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.7480549812316895; mae of 4.7480549812316895\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.386902332305908; mae of 4.386902332305908\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.349487781524658; mae of 4.349487781524658\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.939673900604248; mae of 3.939673900604248\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.487316131591797; mae of 4.487316131591797\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 7.769536018371582 - MAE: 7.769536018371582\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.313778877258301 - MAE: 6.313778877258301\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.504204273223877 - MAE: 5.504204273223877\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.686505317687988 - MAE: 4.686505317687988\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.909460544586182 - MAE: 4.909460544586182\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.7480549812316895 - MAE: 4.7480549812316895\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.386902332305908 - MAE: 4.386902332305908\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.349487781524658 - MAE: 4.349487781524658\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.939673900604248 - MAE: 3.939673900604248\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.487316131591797 - MAE: 4.487316131591797\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.109492015838623 (+- 1.0902671834468338)\n",
      "> Loss: 5.109492015838623\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, BDEs\n",
    "model = build_model_3(11505)\n",
    "inputs = np.array(data_soap_se_noBDEs)\n",
    "mae_per_fold_soap_se_noBDEs_3, loss_per_fold_soap_se_noBDEs_3 = K_fold_cross_validation(\n",
    "    inputs, data_soap_se_noBDEs_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 7.9345903396606445; mae of 7.9345903396606445\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.316888332366943; mae of 6.316888332366943\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.694052696228027; mae of 5.694052696228027\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.833024501800537; mae of 4.833024501800537\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 5.199148654937744; mae of 5.199148654937744\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.351123332977295; mae of 4.351123332977295\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.179715633392334; mae of 4.179715633392334\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.728020191192627; mae of 4.728020191192627\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.8876779079437256; mae of 3.8876779079437256\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.517024993896484; mae of 4.517024993896484\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 7.9345903396606445 - MAE: 7.9345903396606445\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.316888332366943 - MAE: 6.316888332366943\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.694052696228027 - MAE: 5.694052696228027\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.833024501800537 - MAE: 4.833024501800537\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 5.199148654937744 - MAE: 5.199148654937744\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.351123332977295 - MAE: 4.351123332977295\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.179715633392334 - MAE: 4.179715633392334\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.728020191192627 - MAE: 4.728020191192627\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.8876779079437256 - MAE: 3.8876779079437256\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.517024993896484 - MAE: 4.517024993896484\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.164126658439637 (+- 1.1517072358397116)\n",
      "> Loss: 5.164126658439637\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors, enthalpy of reaction\n",
    "model = build_model_3(11506)\n",
    "inputs = np.array(data_soap_DeltaH)\n",
    "mae_per_fold_soap_DeltaH_3, loss_per_fold_soap_DeltaH_3 = K_fold_cross_validation(\n",
    "    inputs, data_soap_DeltaH_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.214994430541992; mae of 8.214994430541992\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.251453876495361; mae of 5.251453876495361\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 4.436524868011475; mae of 4.436524868011475\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.394689559936523; mae of 4.394689559936523\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.332404613494873; mae of 4.332404613494873\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.203233242034912; mae of 4.203233242034912\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 3.7917001247406006; mae of 3.7917001247406006\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 3.486187696456909; mae of 3.486187696456909\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.6278157234191895; mae of 3.6278157234191895\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.27629280090332; mae of 4.27629280090332\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.214994430541992 - MAE: 8.214994430541992\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.251453876495361 - MAE: 5.251453876495361\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 4.436524868011475 - MAE: 4.436524868011475\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.394689559936523 - MAE: 4.394689559936523\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.332404613494873 - MAE: 4.332404613494873\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.203233242034912 - MAE: 4.203233242034912\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.7917001247406006 - MAE: 3.7917001247406006\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 3.486187696456909 - MAE: 3.486187696456909\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.6278157234191895 - MAE: 3.6278157234191895\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.27629280090332 - MAE: 4.27629280090332\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.601529693603515 (+- 1.2936118539714385)\n",
      "> Loss: 4.601529693603515\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LMBTR descriptors, Mordred descriptors\n",
    "model = build_model_3(8239)\n",
    "inputs = np.array(data_lmbtr_se_noBDEs)\n",
    "mae_per_fold_lmbtr_se_noBDEs_3, loss_per_fold_lmbtr_se_noBDEs_3 = K_fold_cross_validation(\n",
    "    inputs, data_lmbtr_se_noBDEs_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.687891006469727; mae of 8.687891006469727\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.695990562438965; mae of 5.695990562438965\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 4.3831071853637695; mae of 4.3831071853637695\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.209273338317871; mae of 4.209273338317871\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.383842468261719; mae of 4.383842468261719\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.087885856628418; mae of 4.087885856628418\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 3.746340274810791; mae of 3.746340274810791\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 3.94338321685791; mae of 3.94338321685791\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.9780900478363037; mae of 3.9780900478363037\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 3.971766233444214; mae of 3.971766233444214\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.687891006469727 - MAE: 8.687891006469727\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.695990562438965 - MAE: 5.695990562438965\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 4.3831071853637695 - MAE: 4.3831071853637695\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.209273338317871 - MAE: 4.209273338317871\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.383842468261719 - MAE: 4.383842468261719\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.087885856628418 - MAE: 4.087885856628418\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.746340274810791 - MAE: 3.746340274810791\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 3.94338321685791 - MAE: 3.94338321685791\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.9780900478363037 - MAE: 3.9780900478363037\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 3.971766233444214 - MAE: 3.971766233444214\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.708757019042968 (+- 1.422806813089318)\n",
      "> Loss: 4.708757019042968\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LMBTR descriptors, Mordred descriptors, enthalpy of reaction\n",
    "model = build_model_3(8240)\n",
    "inputs = np.array(data_lmbtr_DeltaH)\n",
    "mae_per_fold_lmbtr_DeltaH_3, loss_per_fold_lmbtr_DeltaH_3 = K_fold_cross_validation(\n",
    "    inputs, data_lmbtr_DeltaH_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyper_opt_more_layers = pd.read_csv('cross_val_scores_hyper_opt_more_layers.csv', index_col=0)\n",
    "\n",
    "new_scores = pd.DataFrame({'Model': [\n",
    "    *['Model 1 (5 layers)' for i in range(len(mae_per_fold_soap_se_nomordred_3))],\n",
    "    *['Model 2 (5 layers)' for i in range(len(mae_per_fold_soap_se_noBDEs_3))],\n",
    "    *['Model 3 (5 layers)' for i in range(len(mae_per_fold_soap_DeltaH_3))],\n",
    "    *['Model 4 (5 layers)' for i in range(len(mae_per_fold_lmbtr_se_noBDEs_3))],\n",
    "    *['Model 5 (5 layers)' for i in range(len(mae_per_fold_lmbtr_DeltaH_3))]\n",
    "], 'MAE [kcal/mol]': [\n",
    "    *mae_per_fold_soap_se_nomordred_3, *mae_per_fold_soap_se_noBDEs_3, *mae_per_fold_soap_DeltaH_3,\n",
    "    *mae_per_fold_lmbtr_se_noBDEs_3, *mae_per_fold_lmbtr_DeltaH_3\n",
    "]})\n",
    "df_hyper_opt_more_layers = pd.concat([df_hyper_opt_more_layers, new_scores])\n",
    "df_hyper_opt_more_layers.to_csv('cross_val_scores_hyper_opt_more_layers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.072370529174805; mae of 8.072370529174805\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.388580799102783; mae of 6.388580799102783\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.644259452819824; mae of 5.644259452819824\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 5.043553829193115; mae of 5.043553829193115\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.987371921539307; mae of 4.987371921539307\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.252241134643555; mae of 4.252241134643555\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.292725563049316; mae of 4.292725563049316\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.403520107269287; mae of 4.403520107269287\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.152981281280518; mae of 4.152981281280518\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.264362812042236; mae of 4.264362812042236\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.072370529174805 - MAE: 8.072370529174805\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.388580799102783 - MAE: 6.388580799102783\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.644259452819824 - MAE: 5.644259452819824\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 5.043553829193115 - MAE: 5.043553829193115\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.987371921539307 - MAE: 4.987371921539307\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.252241134643555 - MAE: 4.252241134643555\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.292725563049316 - MAE: 4.292725563049316\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.403520107269287 - MAE: 4.403520107269287\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.152981281280518 - MAE: 4.152981281280518\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.264362812042236 - MAE: 4.264362812042236\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.150196743011475 (+- 1.1930182240644795)\n",
      "> Loss: 5.150196743011475\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors\n",
    "model = build_model_4(11491)\n",
    "inputs = np.array(data_soap_se_nomordred)\n",
    "mae_per_fold_soap_se_nomordred_4, loss_per_fold_soap_se_nomordred_4 = K_fold_cross_validation(\n",
    "    inputs, data_soap_se_nomordred_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.082093238830566; mae of 8.082093238830566\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.57771110534668; mae of 6.57771110534668\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.848302841186523; mae of 5.848302841186523\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 5.066501617431641; mae of 5.066501617431641\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 5.848586559295654; mae of 5.848586559295654\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.4897613525390625; mae of 4.4897613525390625\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.22063684463501; mae of 4.22063684463501\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.214164733886719; mae of 4.214164733886719\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.300479888916016; mae of 4.300479888916016\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.454226016998291; mae of 4.454226016998291\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.082093238830566 - MAE: 8.082093238830566\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.57771110534668 - MAE: 6.57771110534668\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.848302841186523 - MAE: 5.848302841186523\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 5.066501617431641 - MAE: 5.066501617431641\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 5.848586559295654 - MAE: 5.848586559295654\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.4897613525390625 - MAE: 4.4897613525390625\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.22063684463501 - MAE: 4.22063684463501\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.214164733886719 - MAE: 4.214164733886719\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.300479888916016 - MAE: 4.300479888916016\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.454226016998291 - MAE: 4.454226016998291\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.310246419906616 (+- 1.2141897329222098)\n",
      "> Loss: 5.310246419906616\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, BDEs\n",
    "model = build_model_4(11505)\n",
    "inputs = np.array(data_soap_se_noBDEs)\n",
    "mae_per_fold_soap_se_noBDEs_4, loss_per_fold_soap_se_noBDEs_4 = K_fold_cross_validation(\n",
    "    inputs, data_soap_se_noBDEs_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.034205436706543; mae of 8.034205436706543\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.620030403137207; mae of 6.620030403137207\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.6546454429626465; mae of 5.6546454429626465\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.735958099365234; mae of 4.735958099365234\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 5.143008232116699; mae of 5.143008232116699\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.523268222808838; mae of 4.523268222808838\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.196821689605713; mae of 4.196821689605713\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.66756534576416; mae of 4.66756534576416\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.099114894866943; mae of 4.099114894866943\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.427176475524902; mae of 4.427176475524902\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.034205436706543 - MAE: 8.034205436706543\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.620030403137207 - MAE: 6.620030403137207\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.6546454429626465 - MAE: 5.6546454429626465\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.735958099365234 - MAE: 4.735958099365234\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 5.143008232116699 - MAE: 5.143008232116699\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.523268222808838 - MAE: 4.523268222808838\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.196821689605713 - MAE: 4.196821689605713\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.66756534576416 - MAE: 4.66756534576416\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.099114894866943 - MAE: 4.099114894866943\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.427176475524902 - MAE: 4.427176475524902\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.210179424285888 (+- 1.1845032511092541)\n",
      "> Loss: 5.210179424285888\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors, enthalpy of reaction\n",
    "model = build_model_4(11506)\n",
    "inputs = np.array(data_soap_DeltaH)\n",
    "mae_per_fold_soap_DeltaH_4, loss_per_fold_soap_DeltaH_4 = K_fold_cross_validation(\n",
    "    inputs, data_soap_DeltaH_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.576237678527832; mae of 8.576237678527832\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.364546775817871; mae of 5.364546775817871\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.0186333656311035; mae of 5.0186333656311035\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.217813491821289; mae of 4.217813491821289\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.273894786834717; mae of 4.273894786834717\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.006100654602051; mae of 4.006100654602051\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.078649520874023; mae of 4.078649520874023\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 3.7438852787017822; mae of 3.7438852787017822\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.8071234226226807; mae of 3.8071234226226807\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 3.9836552143096924; mae of 3.9836552143096924\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.576237678527832 - MAE: 8.576237678527832\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.364546775817871 - MAE: 5.364546775817871\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.0186333656311035 - MAE: 5.0186333656311035\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.217813491821289 - MAE: 4.217813491821289\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.273894786834717 - MAE: 4.273894786834717\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.006100654602051 - MAE: 4.006100654602051\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.078649520874023 - MAE: 4.078649520874023\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 3.7438852787017822 - MAE: 3.7438852787017822\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.8071234226226807 - MAE: 3.8071234226226807\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 3.9836552143096924 - MAE: 3.9836552143096924\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.707054018974304 (+- 1.3810882152510449)\n",
      "> Loss: 4.707054018974304\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LMBTR descriptors, Mordred descriptors\n",
    "model = build_model_4(8239)\n",
    "inputs = np.array(data_lmbtr_se_noBDEs)\n",
    "mae_per_fold_lmbtr_se_noBDEs_4, loss_per_fold_lmbtr_se_noBDEs_4 = K_fold_cross_validation(\n",
    "    inputs, data_lmbtr_se_noBDEs_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.787492752075195; mae of 8.787492752075195\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.272426605224609; mae of 5.272426605224609\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 4.747499942779541; mae of 4.747499942779541\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.104258060455322; mae of 4.104258060455322\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.679937362670898; mae of 4.679937362670898\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 3.914050579071045; mae of 3.914051055908203\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 3.707310676574707; mae of 3.707310676574707\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 3.6876511573791504; mae of 3.6876511573791504\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.619706392288208; mae of 3.619706392288208\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.1997151374816895; mae of 4.1997151374816895\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.787492752075195 - MAE: 8.787492752075195\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.272426605224609 - MAE: 5.272426605224609\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 4.747499942779541 - MAE: 4.747499942779541\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.104258060455322 - MAE: 4.104258060455322\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.679937362670898 - MAE: 4.679937362670898\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 3.914050579071045 - MAE: 3.914051055908203\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.707310676574707 - MAE: 3.707310676574707\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 3.6876511573791504 - MAE: 3.6876511573791504\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.619706392288208 - MAE: 3.619706392288208\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.1997151374816895 - MAE: 4.1997151374816895\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.672004914283752 (+- 1.4639097109505084)\n",
      "> Loss: 4.672004866600036\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LMBTR descriptors, Mordred descriptors, enthalpy of reaction\n",
    "model = build_model_4(8240)\n",
    "inputs = np.array(data_lmbtr_DeltaH)\n",
    "mae_per_fold_lmbtr_DeltaH_4, loss_per_fold_lmbtr_DeltaH_4 = K_fold_cross_validation(\n",
    "    inputs, data_lmbtr_DeltaH_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyper_opt_more_layers = pd.read_csv('cross_val_scores_hyper_opt_more_layers.csv', index_col=0)\n",
    "\n",
    "new_scores = pd.DataFrame({'Model': [\n",
    "    *['Model 1 (6 layers)' for i in range(len(mae_per_fold_soap_se_nomordred_4))],\n",
    "    *['Model 2 (6 layers)' for i in range(len(mae_per_fold_soap_se_noBDEs_4))],\n",
    "    *['Model 3 (6 layers)' for i in range(len(mae_per_fold_soap_DeltaH_4))],\n",
    "    *['Model 4 (6 layers)' for i in range(len(mae_per_fold_lmbtr_se_noBDEs_4))],\n",
    "    *['Model 5 (6 layers)' for i in range(len(mae_per_fold_lmbtr_DeltaH_4))]\n",
    "], 'MAE [kcal/mol]': [\n",
    "    *mae_per_fold_soap_se_nomordred_4, *mae_per_fold_soap_se_noBDEs_4, *mae_per_fold_soap_DeltaH_4,\n",
    "    *mae_per_fold_lmbtr_se_noBDEs_4, *mae_per_fold_lmbtr_DeltaH_4\n",
    "]})\n",
    "df_hyper_opt_more_layers = pd.concat([df_hyper_opt_more_layers, new_scores])\n",
    "df_hyper_opt_more_layers.to_csv('cross_val_scores_hyper_opt_more_layers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.335159301757812; mae of 8.335159301757812\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.26959228515625; mae of 6.26959228515625\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.6732683181762695; mae of 5.6732683181762695\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 5.1814069747924805; mae of 5.1814069747924805\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 5.159209728240967; mae of 5.159209728240967\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.371696472167969; mae of 4.371696472167969\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.144062042236328; mae of 4.144062042236328\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.19075345993042; mae of 4.19075345993042\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.281374454498291; mae of 4.281374454498291\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.45485258102417; mae of 4.45485258102417\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.335159301757812 - MAE: 8.335159301757812\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.26959228515625 - MAE: 6.26959228515625\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.6732683181762695 - MAE: 5.6732683181762695\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 5.1814069747924805 - MAE: 5.1814069747924805\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 5.159209728240967 - MAE: 5.159209728240967\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.371696472167969 - MAE: 4.371696472167969\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.144062042236328 - MAE: 4.144062042236328\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.19075345993042 - MAE: 4.19075345993042\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.281374454498291 - MAE: 4.281374454498291\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.45485258102417 - MAE: 4.45485258102417\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.206137561798096 (+- 1.2416785053879351)\n",
      "> Loss: 5.206137561798096\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors\n",
    "model = build_model_5(11491)\n",
    "inputs = np.array(data_soap_se_nomordred)\n",
    "mae_per_fold_soap_se_nomordred_5, loss_per_fold_soap_se_nomordred_5 = K_fold_cross_validation(\n",
    "    inputs, data_soap_se_nomordred_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.268417358398438; mae of 8.268417358398438\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.487494468688965; mae of 6.487494468688965\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.577263355255127; mae of 5.577263355255127\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.843913555145264; mae of 4.843913555145264\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 5.77504825592041; mae of 5.77504825592041\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.6505537033081055; mae of 4.6505537033081055\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.021173000335693; mae of 4.021173000335693\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.4192047119140625; mae of 4.4192047119140625\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.008975982666016; mae of 4.008975982666016\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.341643810272217; mae of 4.341643810272217\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.268417358398438 - MAE: 8.268417358398438\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.487494468688965 - MAE: 6.487494468688965\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.577263355255127 - MAE: 5.577263355255127\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.843913555145264 - MAE: 4.843913555145264\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 5.77504825592041 - MAE: 5.77504825592041\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.6505537033081055 - MAE: 4.6505537033081055\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.021173000335693 - MAE: 4.021173000335693\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.4192047119140625 - MAE: 4.4192047119140625\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.008975982666016 - MAE: 4.008975982666016\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.341643810272217 - MAE: 4.341643810272217\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.23936882019043 (+- 1.269394064605313)\n",
      "> Loss: 5.23936882019043\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, BDEs\n",
    "model = build_model_5(11505)\n",
    "inputs = np.array(data_soap_se_noBDEs)\n",
    "mae_per_fold_soap_se_noBDEs_5, loss_per_fold_soap_se_noBDEs_5 = K_fold_cross_validation(\n",
    "    inputs, data_soap_se_noBDEs_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.583242416381836; mae of 8.583242416381836\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.170719146728516; mae of 6.170719146728516\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.619753360748291; mae of 5.619753360748291\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.66050910949707; mae of 4.66050910949707\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 5.299604415893555; mae of 5.299604415893555\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.5752997398376465; mae of 4.5752997398376465\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.365913391113281; mae of 4.365913391113281\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.684886932373047; mae of 4.684886932373047\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.271956443786621; mae of 4.271956443786621\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.344784259796143; mae of 4.344784259796143\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.583242416381836 - MAE: 8.583242416381836\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.170719146728516 - MAE: 6.170719146728516\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.619753360748291 - MAE: 5.619753360748291\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.66050910949707 - MAE: 4.66050910949707\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 5.299604415893555 - MAE: 5.299604415893555\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.5752997398376465 - MAE: 4.5752997398376465\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.365913391113281 - MAE: 4.365913391113281\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.684886932373047 - MAE: 4.684886932373047\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.271956443786621 - MAE: 4.271956443786621\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.344784259796143 - MAE: 4.344784259796143\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.2576669216156 (+- 1.2560448112602385)\n",
      "> Loss: 5.2576669216156\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors, enthalpy of reaction\n",
    "model = build_model_5(11506)\n",
    "inputs = np.array(data_soap_DeltaH)\n",
    "mae_per_fold_soap_DeltaH_5, loss_per_fold_soap_DeltaH_5 = K_fold_cross_validation(\n",
    "    inputs, data_soap_DeltaH_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.618277549743652; mae of 8.618277549743652\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.51271915435791; mae of 5.51271915435791\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 4.869907855987549; mae of 4.869907855987549\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.265510082244873; mae of 4.265510082244873\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.483770370483398; mae of 4.483770370483398\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.739161968231201; mae of 4.739161968231201\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 3.824122190475464; mae of 3.824122190475464\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 3.7293334007263184; mae of 3.7293334007263184\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.010531902313232; mae of 4.010531902313232\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.374873161315918; mae of 4.374873161315918\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.618277549743652 - MAE: 8.618277549743652\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.51271915435791 - MAE: 5.51271915435791\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 4.869907855987549 - MAE: 4.869907855987549\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.265510082244873 - MAE: 4.265510082244873\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.483770370483398 - MAE: 4.483770370483398\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.739161968231201 - MAE: 4.739161968231201\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.824122190475464 - MAE: 3.824122190475464\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 3.7293334007263184 - MAE: 3.7293334007263184\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.010531902313232 - MAE: 4.010531902313232\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.374873161315918 - MAE: 4.374873161315918\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.842820763587952 (+- 1.3552048575504818)\n",
      "> Loss: 4.842820763587952\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LMBTR descriptors, Mordred descriptors\n",
    "model = build_model_5(8239)\n",
    "inputs = np.array(data_lmbtr_se_noBDEs)\n",
    "mae_per_fold_lmbtr_se_noBDEs_5, loss_per_fold_lmbtr_se_noBDEs_5 = K_fold_cross_validation(\n",
    "    inputs, data_lmbtr_se_noBDEs_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.943923950195312; mae of 8.943923950195312\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.341494560241699; mae of 5.341494560241699\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 4.954117774963379; mae of 4.954117774963379\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.273805618286133; mae of 4.273805618286133\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.9594502449035645; mae of 4.9594502449035645\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.296631813049316; mae of 4.296631813049316\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.196102619171143; mae of 4.196102619171143\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.078256130218506; mae of 4.078256130218506\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.7627031803131104; mae of 3.7627031803131104\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.177807807922363; mae of 4.177807807922363\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.943923950195312 - MAE: 8.943923950195312\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.341494560241699 - MAE: 5.341494560241699\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 4.954117774963379 - MAE: 4.954117774963379\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.273805618286133 - MAE: 4.273805618286133\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.9594502449035645 - MAE: 4.9594502449035645\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.296631813049316 - MAE: 4.296631813049316\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.196102619171143 - MAE: 4.196102619171143\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.078256130218506 - MAE: 4.078256130218506\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.7627031803131104 - MAE: 3.7627031803131104\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.177807807922363 - MAE: 4.177807807922363\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.898429369926452 (+- 1.4246609933393717)\n",
      "> Loss: 4.898429369926452\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LMBTR descriptors, Mordred descriptors, enthalpy of reaction\n",
    "model = build_model_5(8240)\n",
    "inputs = np.array(data_lmbtr_DeltaH)\n",
    "mae_per_fold_lmbtr_DeltaH_5, loss_per_fold_lmbtr_DeltaH_5 = K_fold_cross_validation(\n",
    "    inputs, data_lmbtr_DeltaH_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyper_opt_more_layers = pd.read_csv('cross_val_scores_hyper_opt_more_layers.csv', index_col=0)\n",
    "\n",
    "new_scores = pd.DataFrame({'Model': [\n",
    "    *['Model 1 (7 layers)' for i in range(len(mae_per_fold_soap_se_nomordred_5))],\n",
    "    *['Model 2 (7 layers)' for i in range(len(mae_per_fold_soap_se_noBDEs_5))],\n",
    "    *['Model 3 (7 layers)' for i in range(len(mae_per_fold_soap_DeltaH_5))],\n",
    "    *['Model 4 (7 layers)' for i in range(len(mae_per_fold_lmbtr_se_noBDEs_5))],\n",
    "    *['Model 5 (7 layers)' for i in range(len(mae_per_fold_lmbtr_DeltaH_5))]\n",
    "], 'MAE [kcal/mol]': [\n",
    "    *mae_per_fold_soap_se_nomordred_5, *mae_per_fold_soap_se_noBDEs_5, *mae_per_fold_soap_DeltaH_5,\n",
    "    *mae_per_fold_lmbtr_se_noBDEs_5, *mae_per_fold_lmbtr_DeltaH_5\n",
    "]})\n",
    "df_hyper_opt_more_layers = pd.concat([df_hyper_opt_more_layers, new_scores])\n",
    "df_hyper_opt_more_layers.to_csv('cross_val_scores_hyper_opt_more_layers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.557238578796387; mae of 8.557238578796387\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.727097511291504; mae of 6.727097511291504\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.869279384613037; mae of 5.869279384613037\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.847896575927734; mae of 4.847896575927734\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 5.704553127288818; mae of 5.704553127288818\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.7884321212768555; mae of 4.7884321212768555\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.670559883117676; mae of 4.670559883117676\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.242371559143066; mae of 4.242371559143066\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.6025261878967285; mae of 4.6025261878967285\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.587113380432129; mae of 4.587113380432129\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.557238578796387 - MAE: 8.557238578796387\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.727097511291504 - MAE: 6.727097511291504\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.869279384613037 - MAE: 5.869279384613037\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.847896575927734 - MAE: 4.847896575927734\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 5.704553127288818 - MAE: 5.704553127288818\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.7884321212768555 - MAE: 4.7884321212768555\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.670559883117676 - MAE: 4.670559883117676\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.242371559143066 - MAE: 4.242371559143066\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.6025261878967285 - MAE: 4.6025261878967285\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.587113380432129 - MAE: 4.587113380432129\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.459706830978393 (+- 1.2591441152245084)\n",
      "> Loss: 5.459706830978393\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors\n",
    "model = build_model_6(11491)\n",
    "inputs = np.array(data_soap_se_nomordred)\n",
    "mae_per_fold_soap_se_nomordred_6, loss_per_fold_soap_se_nomordred_6 = K_fold_cross_validation(\n",
    "    inputs, data_soap_se_nomordred_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.146730422973633; mae of 8.146730422973633\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.420041084289551; mae of 6.420041084289551\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.943288326263428; mae of 5.943288326263428\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.93197774887085; mae of 4.93197774887085\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 5.789530277252197; mae of 5.789530277252197\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.934604644775391; mae of 4.934604644775391\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.341798305511475; mae of 4.341798305511475\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.792391300201416; mae of 4.792391300201416\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.9389350414276123; mae of 3.9389350414276123\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.500188827514648; mae of 4.500188827514648\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.146730422973633 - MAE: 8.146730422973633\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.420041084289551 - MAE: 6.420041084289551\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.943288326263428 - MAE: 5.943288326263428\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.93197774887085 - MAE: 4.93197774887085\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 5.789530277252197 - MAE: 5.789530277252197\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.934604644775391 - MAE: 4.934604644775391\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.341798305511475 - MAE: 4.341798305511475\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.792391300201416 - MAE: 4.792391300201416\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.9389350414276123 - MAE: 3.9389350414276123\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.500188827514648 - MAE: 4.500188827514648\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.37394859790802 (+- 1.178732747435642)\n",
      "> Loss: 5.37394859790802\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, BDEs\n",
    "model = build_model_6(11505)\n",
    "inputs = np.array(data_soap_se_noBDEs)\n",
    "mae_per_fold_soap_se_noBDEs_6, loss_per_fold_soap_se_noBDEs_6 = K_fold_cross_validation(\n",
    "    inputs, data_soap_se_noBDEs_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.50295639038086; mae of 8.50295639038086\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.716197490692139; mae of 6.716197490692139\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.800463676452637; mae of 5.800463676452637\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.739253044128418; mae of 4.739253044128418\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 5.41719388961792; mae of 5.41719388961792\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.413856506347656; mae of 4.413856506347656\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.446105003356934; mae of 4.446105003356934\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.258550643920898; mae of 4.258550643920898\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.507516860961914; mae of 4.507516860961914\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.493022918701172; mae of 4.493022918701172\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.50295639038086 - MAE: 8.50295639038086\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.716197490692139 - MAE: 6.716197490692139\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.800463676452637 - MAE: 5.800463676452637\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.739253044128418 - MAE: 4.739253044128418\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 5.41719388961792 - MAE: 5.41719388961792\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.413856506347656 - MAE: 4.413856506347656\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.446105003356934 - MAE: 4.446105003356934\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.258550643920898 - MAE: 4.258550643920898\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.507516860961914 - MAE: 4.507516860961914\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.493022918701172 - MAE: 4.493022918701172\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.329511642456055 (+- 1.2927767132016463)\n",
      "> Loss: 5.329511642456055\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors, enthalpy of reaction\n",
    "model = build_model_6(11506)\n",
    "inputs = np.array(data_soap_DeltaH)\n",
    "mae_per_fold_soap_DeltaH_6, loss_per_fold_soap_DeltaH_6 = K_fold_cross_validation(\n",
    "    inputs, data_soap_DeltaH_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.803272247314453; mae of 8.803272247314453\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 6.094285011291504; mae of 6.094285011291504\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.468090057373047; mae of 5.468090057373047\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.543014049530029; mae of 4.543014049530029\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.4280829429626465; mae of 4.4280829429626465\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.565420627593994; mae of 4.565420627593994\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 4.392765998840332; mae of 4.392765998840332\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 3.8349697589874268; mae of 3.8349697589874268\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.858198404312134; mae of 3.858198404312134\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.227751731872559; mae of 4.227751731872559\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.803272247314453 - MAE: 8.803272247314453\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 6.094285011291504 - MAE: 6.094285011291504\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.468090057373047 - MAE: 5.468090057373047\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.543014049530029 - MAE: 4.543014049530029\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.4280829429626465 - MAE: 4.4280829429626465\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.565420627593994 - MAE: 4.565420627593994\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 4.392765998840332 - MAE: 4.392765998840332\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 3.8349697589874268 - MAE: 3.8349697589874268\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.858198404312134 - MAE: 3.858198404312134\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.227751731872559 - MAE: 4.227751731872559\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.021585083007812 (+- 1.4222404857482411)\n",
      "> Loss: 5.021585083007812\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LMBTR descriptors, Mordred descriptors\n",
    "model = build_model_6(8239)\n",
    "inputs = np.array(data_lmbtr_se_noBDEs)\n",
    "mae_per_fold_lmbtr_se_noBDEs_6, loss_per_fold_lmbtr_se_noBDEs_6 = K_fold_cross_validation(\n",
    "    inputs, data_lmbtr_se_noBDEs_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.428010940551758; mae of 8.428010940551758\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.674784183502197; mae of 5.674784183502197\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 4.787662982940674; mae of 4.787662982940674\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.421676158905029; mae of 4.421676158905029\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.165317535400391; mae of 4.165317535400391\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.093502521514893; mae of 4.093502521514893\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 3.9659366607666016; mae of 3.9659366607666016\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.211404800415039; mae of 4.211404800415039\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.9424407482147217; mae of 3.9424407482147217\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 3.8318090438842773; mae of 3.8318090438842773\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.428010940551758 - MAE: 8.428010940551758\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.674784183502197 - MAE: 5.674784183502197\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 4.787662982940674 - MAE: 4.787662982940674\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.421676158905029 - MAE: 4.421676158905029\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.165317535400391 - MAE: 4.165317535400391\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.093502521514893 - MAE: 4.093502521514893\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.9659366607666016 - MAE: 3.9659366607666016\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.211404800415039 - MAE: 4.211404800415039\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.9424407482147217 - MAE: 3.9424407482147217\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 3.8318090438842773 - MAE: 3.8318090438842773\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.752254557609558 (+- 1.3290998340658808)\n",
      "> Loss: 4.752254557609558\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LMBTR descriptors, Mordred descriptors, enthalpy of reaction\n",
    "model = build_model_6(8240)\n",
    "inputs = np.array(data_lmbtr_DeltaH)\n",
    "mae_per_fold_lmbtr_DeltaH_6, loss_per_fold_lmbtr_DeltaH_6 = K_fold_cross_validation(\n",
    "    inputs, data_lmbtr_DeltaH_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyper_opt_more_layers = pd.read_csv('cross_val_scores_hyper_opt_more_layers.csv', index_col=0)\n",
    "\n",
    "new_scores = pd.DataFrame({'Model': [\n",
    "    *['Model 1 (8 layers)' for i in range(len(mae_per_fold_soap_se_nomordred_6))],\n",
    "    *['Model 2 (8 layers)' for i in range(len(mae_per_fold_soap_se_noBDEs_6))],\n",
    "    *['Model 3 (8 layers)' for i in range(len(mae_per_fold_soap_DeltaH_6))],\n",
    "    *['Model 4 (8 layers)' for i in range(len(mae_per_fold_lmbtr_se_noBDEs_6))],\n",
    "    *['Model 5 (8 layers)' for i in range(len(mae_per_fold_lmbtr_DeltaH_6))]\n",
    "], 'MAE [kcal/mol]': [\n",
    "    *mae_per_fold_soap_se_nomordred_6, *mae_per_fold_soap_se_noBDEs_6, *mae_per_fold_soap_DeltaH_6,\n",
    "    *mae_per_fold_lmbtr_se_noBDEs_6, *mae_per_fold_lmbtr_DeltaH_6\n",
    "]})\n",
    "df_hyper_opt_more_layers = pd.concat([df_hyper_opt_more_layers, new_scores])\n",
    "df_hyper_opt_more_layers.to_csv('cross_val_scores_hyper_opt_more_layers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the optimal number of units in the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypermodel with 4 layers\n",
    "def model_builder_4layers(hp):\n",
    "  model = tf.keras.Sequential()\n",
    "\n",
    "  # Tune the number of units\n",
    "  hp_units_1 = hp.Int('units_1', min_value=32, max_value=1024, step=32)\n",
    "  hp_units_2 = hp.Int('units_2', min_value=32, max_value=512, step=32)\n",
    "  hp_units_3 = hp.Int('units_3', min_value=32, max_value=256, step=32)\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units_1, activation='relu'))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units_2, activation='relu'))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units_3, activation='relu'))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.001, decay_steps=inputs.shape[0]*0.8/32*1000, decay_rate=1, staircase=False\n",
    "  )\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "  model.compile(optimizer=optimizer, loss='mae', metrics=['mae'])\n",
    "\n",
    "  return model\n",
    "\n",
    "# hypermodel with 5 layers\n",
    "def model_builder_5layers(hp):\n",
    "  model = tf.keras.Sequential()\n",
    "\n",
    "  # Tune the number of units\n",
    "  hp_units_1 = hp.Int('units_1', min_value=32, max_value=1024, step=32)\n",
    "  hp_units_2 = hp.Int('units_2', min_value=32, max_value=512, step=32)\n",
    "  hp_units_3 = hp.Int('units_3', min_value=32, max_value=256, step=32)\n",
    "  hp_units_4 = hp.Int('units_4', min_value=32, max_value=128, step=32)\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units_1, activation='relu'))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units_2, activation='relu'))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units_3, activation='relu'))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units_4, activation='relu'))\n",
    "  model.add(tf.keras.layers.BatchNormalization())\n",
    "  model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "  lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.001, decay_steps=inputs.shape[0]*0.8/32*1000, decay_rate=1, staircase=False\n",
    "  )\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "  model.compile(optimizer=optimizer, loss='mae', metrics=['mae'])\n",
    "\n",
    "  return model\n",
    "\n",
    "# class to perform 10 fold crossvalidated hyperparameter optimization\n",
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "\n",
    "  def run_trial(self, trial, inputs, targets, batch_size=32, epochs=30):\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state = 1)\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(monitor='mae', patience=20)\n",
    "    val_losses = []\n",
    "\n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "      model = self.hypermodel.build(trial.hyperparameters)\n",
    "      model.fit(inputs[train], targets[train], batch_size=batch_size, epochs=epochs, callbacks=callbacks, verbose=0)\n",
    "      val_losses.append(model.evaluate(inputs[test], targets[test]))\n",
    "\n",
    "    self.oracle.update_trial(trial.trial_id, {'val_loss': np.mean(val_losses)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 Complete [00h 02m 12s]\n",
      "val_loss: 8.214355754852296\n",
      "\n",
      "Best val_loss So Far: 8.009508085250854\n",
      "Total elapsed time: 00h 14m 17s\n",
      "\n",
      "Search: Running Trial #8\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "units_1           |416               |160               \n",
      "units_2           |768               |992               \n",
      "units_3           |672               |256               \n",
      "units_4           |160               |256               \n",
      "tuner/epochs      |2                 |2                 \n",
      "tuner/initial_e...|0                 |0                 \n",
      "tuner/bracket     |4                 |4                 \n",
      "tuner/round       |0                 |0                 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "173/173 [==============================] - 1s 3ms/step - loss: 28.4271 - mae: 28.4271\n",
      "Epoch 2/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 13.3074 - mae: 13.3074\n",
      "Epoch 3/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 11.4809 - mae: 11.4809\n",
      "Epoch 4/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 10.0284 - mae: 10.0284\n",
      "Epoch 5/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 9.2632 - mae: 9.2632\n",
      "Epoch 6/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 9.0560 - mae: 9.0560\n",
      "Epoch 7/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 8.5802 - mae: 8.5802\n",
      "Epoch 8/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 7.8591 - mae: 7.8591\n",
      "Epoch 9/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 7.2601 - mae: 7.2601\n",
      "Epoch 10/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 7.4549 - mae: 7.4549\n",
      "Epoch 11/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 7.2488 - mae: 7.2488\n",
      "Epoch 12/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 6.8311 - mae: 6.8311\n",
      "Epoch 13/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 6.3747 - mae: 6.3747\n",
      "Epoch 14/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 6.3577 - mae: 6.3577\n",
      "Epoch 15/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 6.2280 - mae: 6.2280\n",
      "Epoch 16/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.9505 - mae: 5.9505\n",
      "Epoch 17/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.5359 - mae: 5.5359\n",
      "Epoch 18/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.3577 - mae: 5.3577\n",
      "Epoch 19/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.2239 - mae: 5.2239\n",
      "Epoch 20/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.2070 - mae: 5.2070\n",
      "Epoch 21/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.1952 - mae: 5.1952\n",
      "Epoch 22/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.9340 - mae: 4.9340\n",
      "Epoch 23/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.8675 - mae: 4.8675\n",
      "Epoch 24/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.0112 - mae: 5.0112\n",
      "Epoch 25/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.6202 - mae: 4.6202\n",
      "Epoch 26/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.9182 - mae: 4.9182\n",
      "Epoch 27/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.3034 - mae: 4.3034\n",
      "Epoch 28/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.2670 - mae: 4.2670\n",
      "Epoch 29/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.0520 - mae: 4.0520\n",
      "Epoch 30/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.4510 - mae: 4.4510\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 7.4656 - mae: 7.4656\n",
      "Epoch 1/30\n",
      "173/173 [==============================] - 1s 3ms/step - loss: 25.2098 - mae: 25.2098\n",
      "Epoch 2/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 12.2664 - mae: 12.2664\n",
      "Epoch 3/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 11.3621 - mae: 11.3621\n",
      "Epoch 4/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 10.1250 - mae: 10.1250\n",
      "Epoch 5/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 9.8252 - mae: 9.8252\n",
      "Epoch 6/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 8.6027 - mae: 8.6027\n",
      "Epoch 7/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 8.2248 - mae: 8.2248\n",
      "Epoch 8/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 7.9230 - mae: 7.9230\n",
      "Epoch 9/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 7.6905 - mae: 7.6905\n",
      "Epoch 10/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 7.2273 - mae: 7.2273\n",
      "Epoch 11/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 6.7313 - mae: 6.7313\n",
      "Epoch 12/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 7.4102 - mae: 7.4102\n",
      "Epoch 13/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 6.6639 - mae: 6.6639\n",
      "Epoch 14/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 6.2418 - mae: 6.2418\n",
      "Epoch 15/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 6.1999 - mae: 6.1999\n",
      "Epoch 16/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.8657 - mae: 5.8657\n",
      "Epoch 17/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.9590 - mae: 5.9590\n",
      "Epoch 18/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.5764 - mae: 5.5764\n",
      "Epoch 19/30\n",
      "173/173 [==============================] - 0s 2ms/step - loss: 5.4045 - mae: 5.4045\n",
      "Epoch 20/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.7020 - mae: 5.7020\n",
      "Epoch 21/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.2774 - mae: 5.2774\n",
      "Epoch 22/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.1984 - mae: 5.1984\n",
      "Epoch 23/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.8610 - mae: 4.8610\n",
      "Epoch 24/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.8484 - mae: 4.8484\n",
      "Epoch 25/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.6457 - mae: 4.6457\n",
      "Epoch 26/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.5678 - mae: 4.5678\n",
      "Epoch 27/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.6399 - mae: 4.6399\n",
      "Epoch 28/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.4926 - mae: 4.4926\n",
      "Epoch 29/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.2980 - mae: 4.2980\n",
      "Epoch 30/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 4.3142 - mae: 4.3142\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 7.9762 - mae: 7.9762\n",
      "Epoch 1/30\n",
      "173/173 [==============================] - 1s 3ms/step - loss: 26.7284 - mae: 26.7284\n",
      "Epoch 2/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 14.0561 - mae: 14.0561\n",
      "Epoch 3/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 10.9878 - mae: 10.9878\n",
      "Epoch 4/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 9.6564 - mae: 9.6564\n",
      "Epoch 5/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 9.6007 - mae: 9.6007\n",
      "Epoch 6/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 8.8443 - mae: 8.8443\n",
      "Epoch 7/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 8.4725 - mae: 8.4725\n",
      "Epoch 8/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 8.3823 - mae: 8.3823\n",
      "Epoch 9/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 7.7816 - mae: 7.7816\n",
      "Epoch 10/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 7.4555 - mae: 7.4555\n",
      "Epoch 11/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 6.8531 - mae: 6.8531\n",
      "Epoch 12/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 6.9518 - mae: 6.9518\n",
      "Epoch 13/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 6.1765 - mae: 6.1765\n",
      "Epoch 14/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 6.0705 - mae: 6.0705\n",
      "Epoch 15/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 6.3596 - mae: 6.3596\n",
      "Epoch 16/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.8920 - mae: 5.8920\n",
      "Epoch 17/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.9899 - mae: 5.9899\n",
      "Epoch 18/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.6770 - mae: 5.6770\n",
      "Epoch 19/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.2534 - mae: 5.2534\n",
      "Epoch 20/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.3300 - mae: 5.3300\n",
      "Epoch 21/30\n",
      "173/173 [==============================] - 0s 3ms/step - loss: 5.5015 - mae: 5.5015\n",
      "Epoch 22/30\n",
      "  1/173 [..............................] - ETA: 0s - loss: 5.2425 - mae: 5.2425"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors\n",
    "inputs = np.array(data_soap_se_nomordred)\n",
    "\n",
    "tuner = CVTuner(\n",
    "  hypermodel=model_builder_5layers,\n",
    "  oracle=kt.oracles.HyperbandOracle(\n",
    "    objective='val_loss',\n",
    "    hyperband_iterations=2,\n",
    "    seed=1,\n",
    "    hyperparameters=None\n",
    "  ),\n",
    "  directory='hyper_opt_log',\n",
    "  project_name='soap_se_nomordred'\n",
    ")\n",
    "\n",
    "tuner.search(inputs, data_soap_se_nomordred_target)\n",
    "\n",
    "best_hps=tuner.get_best_hyperparameters()[0]\n",
    "print('Optimal number of units are {}, {}, {}.'.format(best_hps.get('units_1'), best_hps.get('units_2'), best_hps.get('units_3')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A corresponding script was run to find the optimal number of units for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate optimized models\n",
    "def build_model4_opt_number(input_shape, no_nodes):\n",
    "    input = tf.keras.layers.Input(shape=(input_shape), dtype='float32')\n",
    "    out = tf.keras.layers.Dense(no_nodes[0], activation='relu')(input)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(no_nodes[1], activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(no_nodes[2], activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(1)(out)\n",
    "    model = tf.keras.Model(input, out)\n",
    "    return model\n",
    "\n",
    "def build_model5_opt_number(input_shape, no_nodes):\n",
    "    input = tf.keras.layers.Input(shape=(input_shape), dtype='float32')\n",
    "    out = tf.keras.layers.Dense(no_nodes[0], activation='relu')(input)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(no_nodes[1], activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(no_nodes[2], activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(no_nodes[3], activation='relu')(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(1)(out)\n",
    "    model = tf.keras.Model(input, out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-13 22:20:08.325824: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-02-13 22:20:08.346096: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3192770000 Hz\n",
      "2022-02-13 22:20:09.231881: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 8.09354019165039; mae of 8.09354019165039\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.9194769859313965; mae of 5.9194769859313965\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.515296936035156; mae of 5.515296936035156\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.445985794067383; mae of 4.445985794067383\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.862082004547119; mae of 4.862082004547119\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.2825751304626465; mae of 4.2825751304626465\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 3.8669004440307617; mae of 3.8669004440307617\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.1050028800964355; mae of 4.1050028800964355\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.9808130264282227; mae of 3.9808130264282227\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.129655361175537; mae of 4.129655361175537\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.09354019165039 - MAE: 8.09354019165039\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.9194769859313965 - MAE: 5.9194769859313965\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.515296936035156 - MAE: 5.515296936035156\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.445985794067383 - MAE: 4.445985794067383\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.862082004547119 - MAE: 4.862082004547119\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.2825751304626465 - MAE: 4.2825751304626465\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.8669004440307617 - MAE: 3.8669004440307617\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.1050028800964355 - MAE: 4.1050028800964355\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.9808130264282227 - MAE: 3.9808130264282227\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.129655361175537 - MAE: 4.129655361175537\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.920132875442505 (+- 1.23850716931829)\n",
      "> Loss: 4.920132875442505\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors\n",
    "model = build_model5_opt_number(11491, [352, 480, 224, 96])\n",
    "inputs = np.array(data_soap_se_nomordred)\n",
    "mae_per_fold_soap_se_nomordred_opt_number, loss_per_fold_soap_se_nomordred_opt_number = K_fold_cross_validation(\n",
    "    inputs, data_soap_se_nomordred_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('mae_per_fold_soap_se_nomordred_opt_number', mae_per_fold_soap_se_nomordred_opt_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 03:34:37.048826: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-02-15 03:34:37.050103: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-02-15 03:34:37.079780: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE: forward compatibility was attempted on non supported HW\n",
      "2022-02-15 03:34:37.079821: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: rh02572.villa-bosch.de\n",
      "2022-02-15 03:34:37.079832: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: rh02572.villa-bosch.de\n",
      "2022-02-15 03:34:37.079945: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.103.1\n",
      "2022-02-15 03:34:37.079984: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.86.0\n",
      "2022-02-15 03:34:37.079995: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 470.86.0 does not match DSO version 470.103.1 -- cannot find working devices in this configuration\n",
      "2022-02-15 03:34:37.080403: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-15 03:34:37.080642: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 03:34:38.090975: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-02-15 03:34:38.122119: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3192770000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 8.033854484558105; mae of 8.033854484558105\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.973505973815918; mae of 5.973505973815918\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.384171009063721; mae of 5.384171009063721\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.39673376083374; mae of 4.39673376083374\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.852954864501953; mae of 4.852954864501953\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.8246893882751465; mae of 4.8246893882751465\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 3.9190831184387207; mae of 3.9190831184387207\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.641083717346191; mae of 4.641083717346191\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 4.094723224639893; mae of 4.094723224639893\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 3.987452268600464; mae of 3.987452268600464\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.033854484558105 - MAE: 8.033854484558105\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.973505973815918 - MAE: 5.973505973815918\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.384171009063721 - MAE: 5.384171009063721\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.39673376083374 - MAE: 4.39673376083374\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.852954864501953 - MAE: 4.852954864501953\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.8246893882751465 - MAE: 4.8246893882751465\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.9190831184387207 - MAE: 3.9190831184387207\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.641083717346191 - MAE: 4.641083717346191\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 4.094723224639893 - MAE: 4.094723224639893\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 3.987452268600464 - MAE: 3.987452268600464\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 5.010825181007386 (+- 1.17714356907885)\n",
      "> Loss: 5.010825181007386\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors, no BDEs\n",
    "model = build_model5_opt_number(11505, [480, 448, 96, 64])\n",
    "inputs = np.array(data_soap_se_noBDEs)\n",
    "mae_per_fold_soap_se_noBDEs_opt_number, loss_per_fold_soap_se_noBDEs_opt_number = K_fold_cross_validation(\n",
    "    inputs, data_soap_se_noBDEs_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('mae_per_fold_soap_se_noBDEs_opt_number', mae_per_fold_soap_se_noBDEs_opt_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 20:11:46.928837: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-02-26 20:11:46.931124: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-02-26 20:11:46.972856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 20:11:46.973267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2060 computeCapability: 7.5\n",
      "coreClock: 1.68GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 312.97GiB/s\n",
      "2022-02-26 20:11:46.973299: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-02-26 20:11:47.328043: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-02-26 20:11:47.328248: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-02-26 20:11:47.520066: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-02-26 20:11:47.695776: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-02-26 20:11:48.014918: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-02-26 20:11:48.239621: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-02-26 20:11:48.633176: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-02-26 20:11:48.633634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 20:11:48.635397: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 20:11:48.636764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-02-26 20:11:48.638704: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-26 20:11:48.639378: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 20:11:48.639826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2060 computeCapability: 7.5\n",
      "coreClock: 1.68GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 312.97GiB/s\n",
      "2022-02-26 20:11:48.639868: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-02-26 20:11:48.639907: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-02-26 20:11:48.639936: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-02-26 20:11:48.639962: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2022-02-26 20:11:48.639986: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2022-02-26 20:11:48.640009: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-02-26 20:11:48.640035: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-02-26 20:11:48.640057: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-02-26 20:11:48.640146: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 20:11:48.640631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 20:11:48.641042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
      "2022-02-26 20:11:48.641079: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2022-02-26 20:11:52.584563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-02-26 20:11:52.584591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
      "2022-02-26 20:11:52.584599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
      "2022-02-26 20:11:52.584845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 20:11:52.585333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 20:11:52.585765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-26 20:11:52.586104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5093 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5)\n",
      "2022-02-26 20:11:52.597455: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 20:11:54.388018: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 203794272 exceeds 10% of free system memory.\n",
      "2022-02-26 20:11:54.525693: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-02-26 20:11:54.543476: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3192740000 Hz\n",
      "2022-02-26 20:11:55.297051: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2022-02-26 20:11:58.804406: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 50948568 exceeds 10% of free system memory.\n",
      "2022-02-26 20:12:14.354983: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 28304760 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 1: loss of 7.934071063995361; mae of 7.934071063995361\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 20:12:15.488158: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 203794272 exceeds 10% of free system memory.\n",
      "2022-02-26 20:12:16.640709: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 50948568 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for fold 2: loss of 5.973849773406982; mae of 5.973849773406982\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 5.430459976196289; mae of 5.430459976196289\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.617372989654541; mae of 4.617372989654541\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.771920680999756; mae of 4.771920680999756\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.298646450042725; mae of 4.298646450042725\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 3.917611837387085; mae of 3.917611837387085\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 4.108444690704346; mae of 4.108444690704346\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.831702470779419; mae of 3.831702470779419\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.638111114501953; mae of 4.638111114501953\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 7.934071063995361 - MAE: 7.934071063995361\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.973849773406982 - MAE: 5.973849773406982\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 5.430459976196289 - MAE: 5.430459976196289\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.617372989654541 - MAE: 4.617372989654541\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.771920680999756 - MAE: 4.771920680999756\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.298646450042725 - MAE: 4.298646450042725\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.917611837387085 - MAE: 3.917611837387085\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 4.108444690704346 - MAE: 4.108444690704346\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.831702470779419 - MAE: 3.831702470779419\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.638111114501953 - MAE: 4.638111114501953\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.952219104766845 (+- 1.1778055923844457)\n",
      "> Loss: 4.952219104766845\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# SOAP descriptors, Mordred descriptors, enthalpy of reaction\n",
    "model = build_model4_opt_number(11506, [704, 448, 128])\n",
    "inputs = np.array(data_soap_DeltaH)\n",
    "\n",
    "mae_per_fold_soap_DeltaH_opt_number, loss_per_fold_soap_DeltaH_opt_number = K_fold_cross_validation(\n",
    "    inputs, data_soap_DeltaH_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('mae_per_fold_soap_DeltaH_opt_number', mae_per_fold_soap_DeltaH_opt_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.558971405029297; mae of 8.558971405029297\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.687483310699463; mae of 5.687483310699463\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 4.775956630706787; mae of 4.775956630706787\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 4.244556903839111; mae of 4.244556903839111\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.668138027191162; mae of 4.668138027191162\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 4.669400691986084; mae of 4.669400691986084\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 3.9728381633758545; mae of 3.9728381633758545\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 3.8195688724517822; mae of 3.8195688724517822\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.5292258262634277; mae of 3.5292258262634277\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 3.727705717086792; mae of 3.727705717086792\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.558971405029297 - MAE: 8.558971405029297\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.687483310699463 - MAE: 5.687483310699463\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 4.775956630706787 - MAE: 4.775956630706787\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 4.244556903839111 - MAE: 4.244556903839111\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.668138027191162 - MAE: 4.668138027191162\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 4.669400691986084 - MAE: 4.669400691986084\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.9728381633758545 - MAE: 3.9728381633758545\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 3.8195688724517822 - MAE: 3.8195688724517822\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.5292258262634277 - MAE: 3.5292258262634277\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 3.727705717086792 - MAE: 3.727705717086792\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.765384554862976 (+- 1.4021117346923995)\n",
      "> Loss: 4.765384554862976\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LMBTR descriptors, Mordred descriptors\n",
    "inputs = np.array(data_lmbtr_se_noBDEs)\n",
    "model = build_model5_opt_number(8239, [736, 192, 96, 96])\n",
    "\n",
    "mae_per_fold_lmbtr_se_noBDEs_opt_number, loss_per_fold_lmbtr_se_noBDEs_opt_number = K_fold_cross_validation(\n",
    "    inputs, data_lmbtr_se_noBDEs_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('mae_per_fold_lmbtr_se_noBDEs_opt_number', mae_per_fold_lmbtr_se_noBDEs_opt_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Score for fold 1: loss of 8.211527824401855; mae of 8.211527824401855\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 2 ...\n",
      "Score for fold 2: loss of 5.153715133666992; mae of 5.153715133666992\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 3 ...\n",
      "Score for fold 3: loss of 4.408202171325684; mae of 4.408202171325684\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Score for fold 4: loss of 3.7796719074249268; mae of 3.7796719074249268\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Score for fold 5: loss of 4.317324161529541; mae of 4.317324161529541\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Score for fold 6: loss of 3.9629247188568115; mae of 3.9629247188568115\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Score for fold 7: loss of 3.708843469619751; mae of 3.708843469619751\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Score for fold 8: loss of 3.642674207687378; mae of 3.642674207687378\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Score for fold 9: loss of 3.8545567989349365; mae of 3.8545567989349365\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Score for fold 10: loss of 4.052886486053467; mae of 4.052886486053467\n",
      "------------------------------------------------------------------------\n",
      "Score per fold\n",
      "------------------------------------------------------------------------\n",
      "> Fold 1 - Loss: 8.211527824401855 - MAE: 8.211527824401855\n",
      "------------------------------------------------------------------------\n",
      "> Fold 2 - Loss: 5.153715133666992 - MAE: 5.153715133666992\n",
      "------------------------------------------------------------------------\n",
      "> Fold 3 - Loss: 4.408202171325684 - MAE: 4.408202171325684\n",
      "------------------------------------------------------------------------\n",
      "> Fold 4 - Loss: 3.7796719074249268 - MAE: 3.7796719074249268\n",
      "------------------------------------------------------------------------\n",
      "> Fold 5 - Loss: 4.317324161529541 - MAE: 4.317324161529541\n",
      "------------------------------------------------------------------------\n",
      "> Fold 6 - Loss: 3.9629247188568115 - MAE: 3.9629247188568115\n",
      "------------------------------------------------------------------------\n",
      "> Fold 7 - Loss: 3.708843469619751 - MAE: 3.708843469619751\n",
      "------------------------------------------------------------------------\n",
      "> Fold 8 - Loss: 3.642674207687378 - MAE: 3.642674207687378\n",
      "------------------------------------------------------------------------\n",
      "> Fold 9 - Loss: 3.8545567989349365 - MAE: 3.8545567989349365\n",
      "------------------------------------------------------------------------\n",
      "> Fold 10 - Loss: 4.052886486053467 - MAE: 4.052886486053467\n",
      "------------------------------------------------------------------------\n",
      "Average scores for all folds:\n",
      "> MAE: 4.509232687950134 (+- 1.3049326861484913)\n",
      "> Loss: 4.509232687950134\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# LMBTR descriptors, Mordred descriptors, enthalpy of reaction\n",
    "inputs = np.array(data_lmbtr_DeltaH)\n",
    "model = build_model4_opt_number(8240, [352, 128, 128])\n",
    "\n",
    "mae_per_fold_lmbtr_DeltaH_opt_number, loss_per_fold_lmbtr_DeltaH_opt_number = K_fold_cross_validation(\n",
    "    inputs, data_lmbtr_DeltaH_target, 10, model, mixed_dtypes=False, loss='mae', no_epochs=30, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('mae_per_fold_lmbtr_DeltaH_opt_number', mae_per_fold_lmbtr_DeltaH_opt_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_4layer_model_with_skipcon(input_shape, no_nodes):\n",
    "    input = tf.keras.layers.Input(shape=(input_shape), dtype='float32')\n",
    "    a = tf.keras.layers.Dense(no_nodes[0], activation='relu')(input)\n",
    "    \n",
    "    b = tf.keras.layers.BatchNormalization()(a)\n",
    "    b = tf.keras.layers.Dense(no_nodes[1], activation='relu')(b)\n",
    "\n",
    "    c = tf.keras.layers.Concatenate()([a, b])\n",
    "    c = tf.keras.layers.BatchNormalization()(c)\n",
    "    c = tf.keras.layers.Dense(no_nodes[2], activation='relu')(c)\n",
    "\n",
    "    d = tf.keras.layers.Concatenate()([a, b, c])\n",
    "    d = tf.keras.layers.BatchNormalization()(d)\n",
    "    out = tf.keras.layers.Dense(1)(d)\n",
    "    model = tf.keras.Model(input, out)\n",
    "    return model\n",
    "\n",
    "def build_5layer_model_with_skipcon(input_shape, no_nodes):\n",
    "    input = tf.keras.layers.Input(shape=(input_shape), dtype='float32')\n",
    "    a = tf.keras.layers.Dense(no_nodes[0], activation='relu')(input)\n",
    "    \n",
    "    b = tf.keras.layers.BatchNormalization()(a)\n",
    "    b = tf.keras.layers.Dense(no_nodes[1], activation='relu')(b)\n",
    "\n",
    "    c = tf.keras.layers.Concatenate()([a, b])\n",
    "    c = tf.keras.layers.BatchNormalization()(c)\n",
    "    c = tf.keras.layers.Dense(no_nodes[2], activation='relu')(c)\n",
    "    \n",
    "    d = tf.keras.layers.Concatenate()([a, b, c])\n",
    "    d = tf.keras.layers.BatchNormalization()(d)\n",
    "    d = tf.keras.layers.Dense(no_nodes[3], activation='relu')(d)\n",
    "\n",
    "    e = tf.keras.layers.Concatenate()([a, b, c, d])\n",
    "    e = tf.keras.layers.BatchNormalization()(e)\n",
    "    out = tf.keras.layers.Dense(1)(e)\n",
    "    model = tf.keras.Model(input, out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skip_NN.png'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = Digraph(\n",
    "    engine='neato',\n",
    "    node_attr={'shape': 'box', 'fontname': 'Roboto Light'},\n",
    "    graph_attr={'fontname': 'Roboto Light', 'splines': 'ortho'}\n",
    ")\n",
    "\n",
    "dot.attr('node', shape='box3d')\n",
    "dot.node('A', 'Input Layer', pos='0,8!')\n",
    "dot.attr('node', shape='box')\n",
    "dot.node('B', 'Hidden Layer', pos='0,7!')\n",
    "dot.node('C', 'Hidden Layer', pos='0,6!')\n",
    "dot.node('D', 'Hidden Layer', pos='0,4!')\n",
    "dot.node('E', 'Hidden Layer', pos='0,2!')\n",
    "dot.attr('node', shape='box3d')\n",
    "dot.node('F', 'Output Layer', pos='0,0!')\n",
    "dot.attr('node', shape='box')\n",
    "\n",
    "#Concatenation nodes\n",
    "dot.attr('node', shape='circle')\n",
    "dot.node('C1', '||', pos='0,5!')\n",
    "dot.node('C2', '||', pos='0,3!')\n",
    "dot.node('C3', '||', pos='0,1!')\n",
    "dot.attr('node', shape='box')\n",
    "\n",
    "dot.edge('A', 'B')\n",
    "dot.edge('B', 'C')\n",
    "dot.edge('B', 'C1')\n",
    "dot.edge('C', 'C1')\n",
    "dot.edge('C1', 'D')\n",
    "dot.edge('B', 'C2')\n",
    "dot.edge('C', 'C2')\n",
    "dot.edge('D', 'C2')\n",
    "dot.edge('C2', 'E')\n",
    "dot.edge('B', 'C3')\n",
    "dot.edge('C', 'C3')\n",
    "dot.edge('D', 'C3')\n",
    "dot.edge('E', 'C3')\n",
    "dot.edge('C3', 'F')\n",
    "\n",
    "dot.format = 'png'\n",
    "dot.render('Skip_NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self-attention\n",
    "class attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_shape=32):\n",
    "        super(attention, self).__init__()\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.batch_size = input_shape[0]\n",
    "        #key weights\n",
    "        self.K = self.add_weight(name='key_weight', shape=(input_shape[1], 128), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        #key bias\n",
    "        self.K_b = self.add_weight(name='key_bias', shape=(self.batch_size, 128), \n",
    "                               initializer='zeros', trainable=True)\n",
    "        #query weights\n",
    "        self.Q = self.add_weight(name='query_weight', shape=(input_shape[1], 128), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        #query bias\n",
    "        self.Q_b = self.add_weight(name='query_bias', shape=(self.batch_size, 128), \n",
    "                               initializer='zeros', trainable=True)\n",
    "        #value weights\n",
    "        self.V = self.add_weight(name='value_weight', shape=(input_shape[1], self.out_shape), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        #value bias\n",
    "        self.V_b = self.add_weight(name='value_bias', shape=(self.batch_size, self.out_shape), \n",
    "                               initializer='zeros', trainable=True)\n",
    "\n",
    "    def call(self,x):\n",
    "        keys = tf.matmul(x, self.K) + self.K_b\n",
    "        queries =  tf.matmul(x, self.Q) + self.Q_b\n",
    "        values = tf.matmul(x, self.V) + self.V_b\n",
    "        #attention scores\n",
    "        attn_scores = K.softmax(tf.matmul(queries, keys, transpose_b=True))\n",
    "        weighted_values = tf.matmul(attn_scores, values)\n",
    "        return weighted_values\n",
    "\n",
    "#multi-head self-attention\n",
    "class multihead_attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_shape=32):\n",
    "        super(multihead_attention, self).__init__()\n",
    "        self.out_shape = out_shape\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.batch_size = input_shape[0]\n",
    "        #key weights\n",
    "        self.K = self.add_weight(name='key_weight', shape=(input_shape[1], 64), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        self.K1 = self.add_weight(name='key_weight1', shape=(64, 64), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        self.K2 = self.add_weight(name='key_weight2', shape=(64, 64), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        self.K3 = self.add_weight(name='key_weight3', shape=(64, 64), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        self.K4 = self.add_weight(name='key_weight4', shape=(64, 64), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        #key bias\n",
    "        # self.K_b = self.add_weight(name='key_bias', shape=(self.batch_size, 128), \n",
    "        #                        initializer='zeros', trainable=True)\n",
    "        #query weights\n",
    "        self.Q = self.add_weight(name='query_weight', shape=(input_shape[1], 64), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        self.Q1 = self.add_weight(name='query_weight1', shape=(64, 64), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        self.Q2 = self.add_weight(name='query_weight2', shape=(64, 64), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        self.Q3 = self.add_weight(name='query_weight3', shape=(64, 64), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        self.Q4 = self.add_weight(name='query_weight4', shape=(64, 64), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        #query bias\n",
    "        # self.Q_b = self.add_weight(name='query_bias', shape=(self.batch_size, 128), \n",
    "        #                        initializer='zeros', trainable=True)\n",
    "        #value weights\n",
    "        self.V = self.add_weight(name='value_weight1', shape=(input_shape[1], self.out_shape), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        self.V1 = self.add_weight(name='value_weight1', shape=(self.out_shape, self.out_shape), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        self.V2 = self.add_weight(name='value_weight2', shape=(self.out_shape, self.out_shape), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        self.V3 = self.add_weight(name='value_weight3', shape=(self.out_shape, self.out_shape), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        self.V4 = self.add_weight(name='value_weight4', shape=(self.out_shape, self.out_shape), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "        #value bias\n",
    "        # self.V_b = self.add_weight(name='value_bias', shape=(self.batch_size, self.out_shape), \n",
    "        #                        initializer='zeros', trainable=True)\n",
    "        #output projection\n",
    "        self.W = self.add_weight(name='output_weight', shape=(4*self.out_shape, self.out_shape), \n",
    "                               initializer='glorot_uniform', trainable=True)\n",
    "\n",
    "    def call(self,x):\n",
    "        keys = tf.matmul(x, self.K)\n",
    "        keys1 = tf.matmul(keys, self.K1)\n",
    "        keys2 = tf.matmul(keys, self.K2)\n",
    "        keys3 = tf.matmul(keys, self.K3)\n",
    "        keys4 = tf.matmul(keys, self.K4)\n",
    "        queries = tf.matmul(x, self.Q)\n",
    "        queries1 =  tf.matmul(queries, self.Q1)\n",
    "        queries2 =  tf.matmul(queries, self.Q2)\n",
    "        queries3 =  tf.matmul(queries, self.Q3)\n",
    "        queries4 =  tf.matmul(queries, self.Q4)\n",
    "        values = tf.matmul(x, self.V)\n",
    "        values1 = tf.matmul(values, self.V1)\n",
    "        values2 = tf.matmul(values, self.V2)\n",
    "        values3 = tf.matmul(values, self.V3)\n",
    "        values4 = tf.matmul(values, self.V4)\n",
    "\n",
    "        #attention scores\n",
    "        attn_scores1 = K.softmax(tf.matmul(queries1, keys1, transpose_b=True))\n",
    "        attn_scores2 = K.softmax(tf.matmul(queries2, keys2, transpose_b=True))\n",
    "        attn_scores3 = K.softmax(tf.matmul(queries3, keys3, transpose_b=True))\n",
    "        attn_scores4 = K.softmax(tf.matmul(queries4, keys4, transpose_b=True))\n",
    "\n",
    "        weighted_values1 = tf.matmul(attn_scores1, values1)\n",
    "        weighted_values2 = tf.matmul(attn_scores2, values2)\n",
    "        weighted_values3 = tf.matmul(attn_scores3, values3)\n",
    "        weighted_values4 = tf.matmul(attn_scores4, values4)\n",
    "\n",
    "        weighted_values_stacked = tf.concat([weighted_values1, weighted_values2, weighted_values3, weighted_values4], axis=1)\n",
    "\n",
    "        weighted_values = tf.matmul(weighted_values_stacked, self.W)\n",
    "\n",
    "        return weighted_values\n",
    "\n",
    "def build_attention_model(size):\n",
    "    input = tf.keras.layers.Input(shape=(size,), dtype='float32', batch_size=41)\n",
    "    embedding_size = size//4\n",
    "    a = tf.keras.layers.Dense(embedding_size)(input)\n",
    "    a = tf.keras.layers.BatchNormalization()(a)\n",
    "    b = attention(embedding_size)(a)\n",
    "    b = tf.keras.layers.BatchNormalization()(b)\n",
    "    c = tf.keras.layers.Dense(128)(b)\n",
    "    c = tf.keras.layers.BatchNormalization()(c)\n",
    "    d = tf.keras.layers.Dense(64)(c)\n",
    "    d = tf.keras.layers.BatchNormalization()(d)\n",
    "    out = tf.keras.layers.Dense(1)(d)\n",
    "    model = tf.keras.Model(input, out)\n",
    "    return model\n",
    "\n",
    "def build_multiheadattention_model(size):\n",
    "    input = tf.keras.layers.Input(shape=(size,), dtype='float32', batch_size=41)\n",
    "    embedding_size = size//4\n",
    "    a = tf.keras.layers.Dense(embedding_size)(input)\n",
    "    a = tf.keras.layers.BatchNormalization()(a)\n",
    "    b = multihead_attention(embedding_size)(a)\n",
    "    b = tf.keras.layers.BatchNormalization()(b)\n",
    "    c = tf.keras.layers.Dense(128)(b)\n",
    "    c = tf.keras.layers.BatchNormalization()(c)\n",
    "    d = tf.keras.layers.Dense(64)(c)\n",
    "    d = tf.keras.layers.BatchNormalization()(d)\n",
    "    out = tf.keras.layers.Dense(1)(d)\n",
    "    model = tf.keras.Model(input, out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_per_fold_soap_se_nomordred_atten = np.load('mae_per_fold_soap_se_nomordred_atten.npy')\n",
    "mae_per_fold_soap_se_nomordred_multih_atten = np.load('mae_per_fold_soap_se_nomordred_multih_atten.npy')\n",
    "mae_per_fold_soap_se_noBDEs_atten = np.load('mae_per_fold_soap_se_noBDEs_atten.npy')\n",
    "mae_per_fold_soap_se_noBDEs_multih_atten = np.load('mae_per_fold_soap_se_noBDEs_multih_atten.npy')\n",
    "mae_per_fold_soap_DeltaH_atten = np.load('mae_per_fold_soap_DeltaH_atten.npy')\n",
    "mae_per_fold_soap_DeltaH_multih_atten = np.load('mae_per_fold_soap_DeltaH_multih_atten.npy')\n",
    "mae_per_fold_lmbtr_se_noBDEs_atten = np.load('mae_per_fold_lmbtr_se_noBDEs_atten.npy')\n",
    "mae_per_fold_lmbtr_se_noBDEs_multih_atten = np.load('mae_per_fold_lmbtr_se_noBDEs_multih_atten.npy')\n",
    "mae_per_fold_lmbtr_DeltaH_atten = np.load('mae_per_fold_lmbtr_DeltaH_atten.npy')\n",
    "mae_per_fold_lmbtr_DeltaH_multih_atten = np.load('mae_per_fold_lmbtr_DeltaH_multih_atten.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.32825126647949 +- 0.5740777877160542\n",
      "25.163867950439453 +- 0.5831178904417817\n",
      "24.837464332580566 +- 0.5375252095028092\n",
      "24.16331787109375 +- 0.4419022135626598\n",
      "25.06620864868164 +- 0.6251650250267081\n",
      "24.655581855773924 +- 0.6270337958096727\n",
      "25.32667407989502 +- 0.6380985052940673\n",
      "24.995582389831544 +- 0.4578610446590041\n",
      "25.3678560256958 +- 0.6919343370797165\n",
      "24.741838264465333 +- 0.6690168319894865\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(mae_per_fold_soap_se_nomordred_atten), '+-', np.std(mae_per_fold_soap_se_nomordred_atten))\n",
    "print(np.mean(mae_per_fold_soap_se_nomordred_multih_atten), '+-', np.std(mae_per_fold_soap_se_nomordred_multih_atten))\n",
    "print(np.mean(mae_per_fold_soap_se_noBDEs_atten), '+-', np.std(mae_per_fold_soap_se_noBDEs_atten))\n",
    "print(np.mean(mae_per_fold_soap_se_noBDEs_multih_atten), '+-', np.std(mae_per_fold_soap_se_noBDEs_multih_atten))\n",
    "print(np.mean(mae_per_fold_soap_DeltaH_atten), '+-', np.std(mae_per_fold_soap_DeltaH_atten))\n",
    "print(np.mean(mae_per_fold_soap_DeltaH_multih_atten), '+-', np.std(mae_per_fold_soap_DeltaH_multih_atten))\n",
    "print(np.mean(mae_per_fold_lmbtr_se_noBDEs_atten), '+-', np.std(mae_per_fold_lmbtr_se_noBDEs_atten))\n",
    "print(np.mean(mae_per_fold_lmbtr_se_noBDEs_multih_atten), '+-', np.std(mae_per_fold_lmbtr_se_noBDEs_multih_atten))\n",
    "print(np.mean(mae_per_fold_lmbtr_DeltaH_atten), '+-', np.std(mae_per_fold_lmbtr_DeltaH_atten))\n",
    "print(np.mean(mae_per_fold_lmbtr_DeltaH_multih_atten), '+-', np.std(mae_per_fold_lmbtr_DeltaH_multih_atten))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Atten_NN_general.png'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = Digraph(\n",
    "    node_attr={'shape': 'box', 'fontname': 'Roboto Light'},\n",
    "    graph_attr={'fontname': 'Roboto Light', 'splines': 'ortho', 'rankdir': 'LR'}\n",
    ")\n",
    "\n",
    "dot.attr('node', shape='box3d')\n",
    "dot.node('A', 'Input Layer')\n",
    "dot.attr('node', shape='box')\n",
    "dot.node('B', 'Hidden Layer\\nN/4 Nodes')\n",
    "dot.node('C', 'Attention')\n",
    "dot.node('D', 'Hidden Layer\\n128 Nodes')\n",
    "dot.node('E', 'Hidden Layer\\n64 Nodes')\n",
    "dot.attr('node', shape='box3d')\n",
    "dot.node('F', 'Output Layer')\n",
    "dot.attr('node', shape='box')\n",
    "\n",
    "dot.edge('A', 'B')\n",
    "dot.edge('B', 'C')\n",
    "dot.edge('C', 'D')\n",
    "dot.edge('D', 'E')\n",
    "dot.edge('E', 'F')\n",
    "\n",
    "dot.format = 'png'\n",
    "dot.render('Atten_NN_general')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the optimal activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model4(input_shape, no_nodes, activation):\n",
    "    input = tf.keras.layers.Input(shape=(input_shape), dtype='float32')\n",
    "    out = tf.keras.layers.Dense(no_nodes[0], activation=activation)(input)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(no_nodes[1], activation=activation)(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(no_nodes[2], activation=activation)(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(1)(out)\n",
    "    model = tf.keras.Model(input, out)\n",
    "    return model\n",
    "\n",
    "def build_model5(input_shape, no_nodes, activation):\n",
    "    input = tf.keras.layers.Input(shape=(input_shape), dtype='float32')\n",
    "    out = tf.keras.layers.Dense(no_nodes[0], activation=activation)(input)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(no_nodes[1], activation=activation)(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(no_nodes[2], activation=activation)(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(no_nodes[3], activation=activation)(out)\n",
    "    out = tf.keras.layers.BatchNormalization()(out)\n",
    "    out = tf.keras.layers.Dense(1)(out)\n",
    "    model = tf.keras.Model(input, out)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "718f978d4e40b555424718481973547209a5ffd9088a1c24c2ec93578d521cf4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('conda_tflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
