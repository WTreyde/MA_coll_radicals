{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Radical Migration in Collagen\n",
    "This notebook contains the code to prepare the data for and train the final PaiNN models for the prediction of the height of hydrogen atom transfer reaction barriers in collagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign BDEs and other descriptors to data points\n",
    "In a first step, we have to assign the correct BDEs and other descriptors to the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from utils import add_ref_idx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import BDEs\n",
    "# names = Chemical name of the amino acid radical\n",
    "# BDE_H, BDE_G = BDE, BDFE\n",
    "# pdb, pdb_H = path to PDB file of radical and reactant\n",
    "# ref_comp = reference compound used in the isodesmic reaction method\n",
    "data = pd.read_csv(\n",
    "    '/hits/fast/mbm/treydewk/optimized_test/BDEs.txt',\n",
    "    sep = '\\t\\t',\n",
    "    names = ['names', 'BDE_H', 'BDE_G', 'charge', 'pdb', 'pdb_H', 'ref_comp'],\n",
    "    engine='python'\n",
    ")\n",
    "\n",
    "### read in manually assigned alternative indices\n",
    "# This need to be done because the matching function returns unique\n",
    "# indices, but several H atoms can be chemically equivalent\n",
    "alt_ind = pd.read_csv(\n",
    "    '/hits/fast/mbm/treydewk/optimized_test/alt_ind.txt',\n",
    "    sep = '\\t\\t',\n",
    "    names = ['names', 'alt_idx'], engine='python'\n",
    ")\n",
    "# the indices returned by the matching function are zero-indexed,\n",
    "# whereas the PDB indices are one-indexed\n",
    "for i in range(0,len(alt_ind['alt_idx'])):\n",
    "    if not alt_ind.iloc[i,1] is None:\n",
    "        ele_list = alt_ind.iloc[i,1].split(sep=',')\n",
    "        for j,k in enumerate(ele_list):\n",
    "            ele_list[j] = int(k)-1 \n",
    "        alt_ind.iloc[i,1] = ele_list\n",
    "\n",
    "### Manual structure matching for Gly_x-1-9.pdb\n",
    "termini_df = data.iloc[[180,182]] # 180 -> 8, 182 -> 10\n",
    "termini_df = termini_df.reset_index(drop=True)\n",
    "ref_termini = [Path('/hits/basement/mbm/riedmiki/structures/KR0008/reference_structures/Gly_x-1-9.pdb'), Path('/hits/basement/mbm/riedmiki/structures/KR0008/reference_structures/Gly_x-1-9.pdb')]\n",
    "idx_termini = [8,10]\n",
    "idx_termini_list = [[8,],[10,]]\n",
    "matched = pd.DataFrame(zip(ref_termini, idx_termini, idx_termini_list), columns=['ref', 'ref_idx', 'alt_idx'])\n",
    "termini_df = termini_df.join(matched)\n",
    "\n",
    "### drop data for neutral arginine [8-13], aspartic acid [19-21], backbone [22], C termini [23-24], glutamic acid [54-57],\n",
    "# N termini [135-137], acetylated and N-amino formylated termini [180-183], cationic histidine [59-64],\n",
    "# pi tautomer of histidine [65-68], hlknl crosslinks [74-85],\n",
    "# uncharged hydroxylysine alpha and beta [93-94], charged hydroxylysine amine radical [92] (computation led to fragmentation),\n",
    "# uncharged lysine [125-130], pyd crosslinks [151-156]\n",
    "# There are no data points and reference PDBs available for these radicals\n",
    "data = data.drop(index=[\n",
    "    8, 9, 10, 11, 12, 13, 19, 20, 21, 22, 23, 24, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
    "    74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 92, 93, 94, 125, 126, 127, 128, 129, 130, 135, 136,\n",
    "    137, 151, 152, 153, 154, 155, 156, 180, 181, 182, 183\n",
    "])\n",
    "alt_ind = alt_ind.drop(index=[\n",
    "    8, 9, 10, 11, 12, 13, 19, 20, 21, 22, 23, 24, 54, 55, 56, 57, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
    "    74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 92, 93, 94, 125, 126, 127, 128, 129, 130, 135, 136,\n",
    "    137, 151, 152, 153, 154, 155, 156, 180, 181, 182, 183\n",
    "])\n",
    "data = data.reset_index(drop=True)\n",
    "alt_ind = alt_ind.reset_index(drop=True)\n",
    "\n",
    "### average BDEs for cis and trans conformers of proline and hydroxyproline\n",
    "# Hyp\n",
    "alpha_cis = data[data['names'] == 'Hyp_cis_alpha']\n",
    "alpha_trans = data[data['names'] == 'Hyp_trans_alpha']\n",
    "pyrr3_cis = data[data['names'] == 'Hyp_cis_pyrr3']\n",
    "pyrr3_trans = data[data['names'] == 'Hyp_trans_pyrr3']\n",
    "pyrr4_onC_cis = data[data['names'] == 'Hyp_cis_pyrr4_onC']\n",
    "pyrr4_onC_trans = data[data['names'] == 'Hyp_trans_pyrr4_onC']\n",
    "pyrr4_onO_cis = data[data['names'] == 'Hyp_cis_pyrr4_onO']\n",
    "pyrr4_onO_trans = data[data['names'] == 'Hyp_trans_pyrr4_onO']\n",
    "pyrr5_cis = data[data['names'] == 'Hyp_cis_pyrr5']\n",
    "pyrr5_trans = data[data['names'] == 'Hyp_trans_pyrr5']\n",
    "\n",
    "alpha_idx = alpha_cis.index[0]\n",
    "pyrr3_idx = pyrr3_cis.index[0]\n",
    "pyrr4_onC_idx = pyrr4_onC_cis.index[0]\n",
    "pyrr4_onO_idx = pyrr4_onO_cis.index[0]\n",
    "pyrr5_idx = pyrr5_cis.index[0]\n",
    "\n",
    "data.iloc[alpha_idx,0] = 'Hyp_alpha'\n",
    "data.iloc[pyrr3_idx,0] = 'Hyp_pyrr3'\n",
    "data.iloc[pyrr4_onC_idx,0] = 'Hyp_pyrr4_onC'\n",
    "data.iloc[pyrr4_onO_idx,0] = 'Hyp_pyrr4_onO'\n",
    "data.iloc[pyrr5_idx,0] = 'Hyp_pyrr5'\n",
    "\n",
    "data.iloc[alpha_idx,1] = (alpha_cis.iloc[0,1] + alpha_trans.iloc[0,1])/2\n",
    "data.iloc[pyrr3_idx,1] = (pyrr3_cis.iloc[0,1] + pyrr3_trans.iloc[0,1])/2\n",
    "data.iloc[pyrr4_onC_idx,1] = (pyrr4_onC_cis.iloc[0,1] + pyrr4_onC_trans.iloc[0,1])/2\n",
    "data.iloc[pyrr4_onO_idx,1] = (pyrr4_onO_cis.iloc[0,1] + pyrr4_onO_trans.iloc[0,1])/2\n",
    "data.iloc[pyrr5_idx,1] = (pyrr5_cis.iloc[0,1] + pyrr5_trans.iloc[0,1])/2\n",
    "\n",
    "data.iloc[alpha_idx,2] = (alpha_cis.iloc[0,2] + alpha_trans.iloc[0,2])/2\n",
    "data.iloc[pyrr3_idx,2] = (pyrr3_cis.iloc[0,2] + pyrr3_trans.iloc[0,2])/2\n",
    "data.iloc[pyrr4_onC_idx,2] = (pyrr4_onC_cis.iloc[0,2] + pyrr4_onC_trans.iloc[0,2])/2\n",
    "data.iloc[pyrr4_onO_idx,2] = (pyrr4_onO_cis.iloc[0,2] + pyrr4_onO_trans.iloc[0,2])/2\n",
    "data.iloc[pyrr5_idx,2] = (pyrr5_cis.iloc[0,2] + pyrr5_trans.iloc[0,2])/2\n",
    "\n",
    "data = data.drop(index=[\n",
    "    alpha_trans.index[0], pyrr3_trans.index[0], pyrr4_onC_trans.index[0], pyrr4_onO_trans.index[0], pyrr5_trans.index[0]\n",
    "])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "alt_ind = alt_ind.drop(index=[\n",
    "    alpha_trans.index[0], pyrr3_trans.index[0], pyrr4_onC_trans.index[0], pyrr4_onO_trans.index[0], pyrr5_trans.index[0]\n",
    "])\n",
    "alt_ind = alt_ind.reset_index(drop=True)\n",
    "\n",
    "# Pro\n",
    "alpha_cis = data[data['names'] == 'Pro_cis_alpha']\n",
    "alpha_trans = data[data['names'] == 'Pro_trans_alpha']\n",
    "pyrr3_cis = data[data['names'] == 'Pro_cis_pyrr3']\n",
    "pyrr3_trans = data[data['names'] == 'Pro_trans_pyrr3']\n",
    "pyrr4_cis = data[data['names'] == 'Pro_cis_pyrr4']\n",
    "pyrr4_trans = data[data['names'] == 'Pro_trans_pyrr4']\n",
    "pyrr5_cis = data[data['names'] == 'Pro_cis_pyrr5']\n",
    "pyrr5_trans = data[data['names'] == 'Pro_trans_pyrr5']\n",
    "\n",
    "alpha_idx = alpha_cis.index[0]\n",
    "pyrr3_idx = pyrr3_cis.index[0]\n",
    "pyrr4_idx = pyrr4_cis.index[0]\n",
    "pyrr5_idx = pyrr5_cis.index[0]\n",
    "\n",
    "data.iloc[alpha_idx,0] = 'Pro_alpha'\n",
    "data.iloc[pyrr3_idx,0] = 'Pro_pyrr3'\n",
    "data.iloc[pyrr4_idx,0] = 'Pro_pyrr4'\n",
    "data.iloc[pyrr5_idx,0] = 'Pro_pyrr5'\n",
    "\n",
    "data.iloc[alpha_idx,1] = (alpha_cis.iloc[0,1] + alpha_trans.iloc[0,1])/2\n",
    "data.iloc[pyrr3_idx,1] = (pyrr3_cis.iloc[0,1] + pyrr3_trans.iloc[0,1])/2\n",
    "data.iloc[pyrr4_idx,1] = (pyrr4_cis.iloc[0,1] + pyrr4_trans.iloc[0,1])/2\n",
    "data.iloc[pyrr5_idx,1] = (pyrr5_cis.iloc[0,1] + pyrr5_trans.iloc[0,1])/2\n",
    "\n",
    "data.iloc[alpha_idx,2] = (alpha_cis.iloc[0,2] + alpha_trans.iloc[0,2])/2\n",
    "data.iloc[pyrr3_idx,2] = (pyrr3_cis.iloc[0,2] + pyrr3_trans.iloc[0,2])/2\n",
    "data.iloc[pyrr4_idx,2] = (pyrr4_cis.iloc[0,2] + pyrr4_trans.iloc[0,2])/2\n",
    "data.iloc[pyrr5_idx,2] = (pyrr5_cis.iloc[0,2] + pyrr5_trans.iloc[0,2])/2\n",
    "\n",
    "data = data.drop(index=[\n",
    "    alpha_trans.index[0], pyrr3_trans.index[0], pyrr4_trans.index[0], pyrr5_trans.index[0]\n",
    "])\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "alt_ind = alt_ind.drop(index=[\n",
    "    alpha_trans.index[0], pyrr3_trans.index[0], pyrr4_trans.index[0], pyrr5_trans.index[0]\n",
    "])\n",
    "alt_ind = alt_ind.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = data.join(data.apply(add_ref_idx, axis=1, result_type=\"expand\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### append manually assigned alternative indices for chemically equivalent H atoms\n",
    "alt_ind.drop(columns=['names',], inplace=True)\n",
    "results = results.join(alt_ind)\n",
    "results = results.append(termini_df)\n",
    "results = results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### let's save our progress\n",
    "results.to_pickle('BDE_df')\n",
    "# results = pd.read_pickle('BDE_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataframe containing HAT reaction barriers\n",
    "df_tidy_idx = pd.read_pickle('/hits/basement/mbm/riedmiki/structures/KR0008/df_tidy_pckl_220118_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to append BDEs to the dataframe containing HAT reaction barriers,\n",
    "# for each row, we will look for the same reference PDB and then the\n",
    "# same index of the reacting H atom. Because there are several PDBs \n",
    "# for a specific amino acid with differently sized capping groups \n",
    "# that have different suffices, we need to only check that the stem\n",
    "# is the same.\n",
    "ref_stem = pd.DataFrame([str(ref.resolve())[:-6] for ref in results['ref']], columns = ['ref_stem',])\n",
    "results = results.join(ref_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating the dataframe with the BDEs for each data point\n",
    "name_rad = []; name_H = []; ref_comp_rad = []; ref_comp_H = []\n",
    "BDEs_sorted_rad = []; BDEs_sorted_H = []; BDEs_G_sorted_rad = []; BDEs_G_sorted_H = []\n",
    "\n",
    "for rad_ref, rad_ref_idx in zip(df_tidy_idx['rad_ref'], df_tidy_idx['rad_ref_idx']):\n",
    "    ref_path = str(rad_ref.resolve())[:-6]\n",
    "    found = results[results['ref_stem']==ref_path]\n",
    "    to_drop = []\n",
    "    for i,l in zip(found.index, found['alt_idx']):\n",
    "        if int(rad_ref_idx) not in l:\n",
    "            to_drop.append(i)\n",
    "    found.drop(to_drop, inplace=True)\n",
    "    if found.shape[0]>0:\n",
    "        idx = found.index[0]\n",
    "        name_rad.append(results.iloc[idx,0])\n",
    "        BDEs_sorted_rad.append(results.iloc[idx,1])\n",
    "        BDEs_G_sorted_rad.append(results.iloc[idx,2])\n",
    "        ref_comp_rad.append(results.iloc[idx,6])\n",
    "    else:\n",
    "        name_rad.append(np.nan)\n",
    "        BDEs_sorted_rad.append(np.nan)\n",
    "        BDEs_G_sorted_rad.append(np.nan)\n",
    "        ref_comp_rad.append(np.nan)\n",
    "\n",
    "for h_ref, h_ref_idx in zip(df_tidy_idx['h_ref'], df_tidy_idx['h_ref_idx']):\n",
    "    ref_path = str(h_ref.resolve())[:-6]\n",
    "    found = results[results['ref_stem']==ref_path]\n",
    "    to_drop = []\n",
    "    for i,l in zip(found.index, found['alt_idx']):\n",
    "        if int(h_ref_idx) not in l:\n",
    "            to_drop.append(i)\n",
    "    found.drop(to_drop, inplace=True)\n",
    "    if found.shape[0]>0:\n",
    "        idx = found.index[0]\n",
    "        name_H.append(results.iloc[idx,0])\n",
    "        BDEs_sorted_H.append(results.iloc[idx,1])\n",
    "        BDEs_G_sorted_H.append(results.iloc[idx,2])\n",
    "        ref_comp_H.append(results.iloc[idx,6])\n",
    "    else:\n",
    "        name_H.append(np.nan)\n",
    "        BDEs_sorted_H.append(np.nan)\n",
    "        BDEs_G_sorted_H.append(np.nan)\n",
    "        ref_comp_H.append(np.nan)\n",
    "\n",
    "BDEs_df = pd.DataFrame(\n",
    "    zip(name_rad, name_H, ref_comp_rad, ref_comp_H, BDEs_sorted_rad, BDEs_sorted_H, BDEs_G_sorted_rad, BDEs_G_sorted_H),\n",
    "    columns = ['rad_chem_name', 'H_chem_name', 'rad_ref_comp', 'H_ref_comp', 'rad_BDE', 'H_BDE', 'rad_BDE_G', 'H_BDE_G']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = df_tidy_idx.join(BDEs_df)\n",
    "# drop data points for which no BDE was found, i.e., HAT after a backbone break\n",
    "complete_dropped = complete.dropna(subset=['rad_BDE'])\n",
    "complete_dropped = complete_dropped.dropna(subset=['H_BDE'])\n",
    "# let's save our progress again\n",
    "complete_dropped.to_pickle('data_complete')\n",
    "# complete_dropped = pd.read_pickle('data_complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now we still have to append the other descriptors\n",
    "BDE_data = pd.read_pickle('BDE_df')\n",
    "\n",
    "rad_BDE = complete_dropped['rad_BDE'].to_list()\n",
    "H_BDE = complete_dropped['H_BDE'].to_list()\n",
    "\n",
    "# PDB files of radical in HAT reactions\n",
    "rad_PDB = []\n",
    "for rad in rad_BDE:\n",
    "    idx = BDE_data[BDE_data['BDE_H'] == rad].index[0]\n",
    "    rad_PDB.append(BDE_data.iloc[idx, 4])\n",
    "\n",
    "# PDB files of H donors in HAT reactions\n",
    "H_PDB = []\n",
    "for H in H_BDE:\n",
    "    idx = BDE_data[BDE_data['BDE_H'] == H].index[0]\n",
    "    H_PDB.append(BDE_data.iloc[idx, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_df = pd.read_csv('/hits/fast/mbm/treydewk/optimized_test/descriptors.csv')\n",
    "# Mordred errors\n",
    "to_drop = []\n",
    "for column in descriptors_df.columns:\n",
    "    if isinstance(descriptors_df[column].to_list()[1], str):\n",
    "        to_drop.append(column)\n",
    "del to_drop[to_drop.index('names')], to_drop[to_drop.index('pdb')], to_drop[to_drop.index('SMILES')]\n",
    "descriptors_df.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "### average descriptors for cis and trans conformers of proline and hydroxyproline\n",
    "# Hyp\n",
    "alpha_cis = descriptors_df[descriptors_df['names'] == 'hyp_cis_pyrr2_1']\n",
    "alpha_trans = descriptors_df[descriptors_df['names'] == 'Hyp_trans_pyrr2_1']\n",
    "pyrr3_cis = descriptors_df[descriptors_df['names'] == 'hyp_cis_pyrr3_1']\n",
    "pyrr3_trans = descriptors_df[descriptors_df['names'] == 'Hyp_trans_pyrr3_1']\n",
    "pyrr4_onC_cis = descriptors_df[descriptors_df['names'] == 'hyp_cis_pyrr4_onC_1']\n",
    "pyrr4_onC_trans = descriptors_df[descriptors_df['names'] == 'Hyp_trans_pyrr4_onC_1']\n",
    "pyrr4_onO_cis = descriptors_df[descriptors_df['names'] == 'hyp_cis_pyrr4_onO_1']\n",
    "pyrr4_onO_trans = descriptors_df[descriptors_df['names'] == 'Hyp_trans_pyrr4_onO_1']\n",
    "pyrr5_cis = descriptors_df[descriptors_df['names'] == 'hyp_cis_pyrr5_1']\n",
    "pyrr5_trans = descriptors_df[descriptors_df['names'] == 'Hyp_trans_pyrr5_1']\n",
    "\n",
    "alpha_idx = alpha_cis.index[0]\n",
    "pyrr3_idx = pyrr3_cis.index[0]\n",
    "pyrr4_onC_idx = pyrr4_onC_cis.index[0]\n",
    "pyrr4_onO_idx = pyrr4_onO_cis.index[0]\n",
    "pyrr5_idx = pyrr5_cis.index[0]\n",
    "\n",
    "for i in range(4,descriptors_df.shape[1]):\n",
    "    descriptors_df.iloc[alpha_idx,i] = (alpha_cis.iloc[0,i] + alpha_trans.iloc[0,i])/2\n",
    "    descriptors_df.iloc[pyrr3_idx,i] = (pyrr3_cis.iloc[0,i] + pyrr3_trans.iloc[0,i])/2\n",
    "    descriptors_df.iloc[pyrr4_onC_idx,i] = (pyrr4_onC_cis.iloc[0,i] + pyrr4_onC_trans.iloc[0,i])/2\n",
    "    descriptors_df.iloc[pyrr4_onO_idx,i] = (pyrr4_onO_cis.iloc[0,i] + pyrr4_onO_trans.iloc[0,i])/2\n",
    "    descriptors_df.iloc[pyrr5_idx,i] = (pyrr5_cis.iloc[0,i] + pyrr5_trans.iloc[0,i])/2\n",
    "\n",
    "descriptors_df = descriptors_df.drop(index=[\n",
    "    alpha_trans.index[0], pyrr3_trans.index[0], pyrr4_onC_trans.index[0], pyrr4_onO_trans.index[0], pyrr5_trans.index[0]\n",
    "])\n",
    "descriptors_df = descriptors_df.reset_index(drop=True)\n",
    "\n",
    "# Pro\n",
    "alpha_cis = descriptors_df[descriptors_df['names'] == 'pro_cis_pyrr2_1']\n",
    "alpha_trans = descriptors_df[descriptors_df['names'] == 'Pro_trans_pyrr2_1']\n",
    "pyrr3_cis = descriptors_df[descriptors_df['names'] == 'pro_cis_pyrr3_1']\n",
    "pyrr3_trans = descriptors_df[descriptors_df['names'] == 'Pro_trans_pyrr3_1']\n",
    "pyrr4_cis = descriptors_df[descriptors_df['names'] == 'pro_cis_pyrr4_1']\n",
    "pyrr4_trans = descriptors_df[descriptors_df['names'] == 'Pro_trans_pyrr4_1']\n",
    "pyrr5_cis = descriptors_df[descriptors_df['names'] == 'pro_cis_pyrr5_1']\n",
    "pyrr5_trans = descriptors_df[descriptors_df['names'] == 'Pro_trans_pyrr5_1']\n",
    "\n",
    "alpha_idx = alpha_cis.index[0]\n",
    "pyrr3_idx = pyrr3_cis.index[0]\n",
    "pyrr4_idx = pyrr4_cis.index[0]\n",
    "pyrr5_idx = pyrr5_cis.index[0]\n",
    "\n",
    "for i in range(4,descriptors_df.shape[1]):\n",
    "    descriptors_df.iloc[alpha_idx,i] = (alpha_cis.iloc[0,i] + alpha_trans.iloc[0,i])/2\n",
    "    descriptors_df.iloc[pyrr3_idx,i] = (pyrr3_cis.iloc[0,i] + pyrr3_trans.iloc[0,i])/2\n",
    "    descriptors_df.iloc[pyrr4_idx,i] = (pyrr4_cis.iloc[0,i] + pyrr4_trans.iloc[0,i])/2\n",
    "    descriptors_df.iloc[pyrr5_idx,i] = (pyrr5_cis.iloc[0,i] + pyrr5_trans.iloc[0,i])/2\n",
    "\n",
    "descriptors_df = descriptors_df.drop(index=[\n",
    "    alpha_trans.index[0], pyrr3_trans.index[0], pyrr4_trans.index[0], pyrr5_trans.index[0]\n",
    "])\n",
    "descriptors_df = descriptors_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_sorted_rad = pd.DataFrame()\n",
    "descriptors_sorted_H = pd.DataFrame()\n",
    "\n",
    "# descriptors for radicals in HAT reactions\n",
    "for pdb in rad_PDB:\n",
    "    idx = descriptors_df[descriptors_df['pdb']==pdb].index[0]\n",
    "    descriptors_sorted_rad = descriptors_sorted_rad.append(descriptors_df.iloc[idx])\n",
    "\n",
    "# descriptors for H donors in HAT reactions\n",
    "for pdb in H_PDB:\n",
    "    idx = descriptors_df[descriptors_df['pdb']==pdb].index[0]\n",
    "    descriptors_sorted_H = descriptors_sorted_H.append(descriptors_df.iloc[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors_sorted_rad = descriptors_sorted_rad.drop(columns = [descriptors_sorted_rad.columns[0], 'names'])\n",
    "descriptors_sorted_H = descriptors_sorted_H.drop(columns = [descriptors_sorted_H.columns[0], 'names'])\n",
    "\n",
    "for column in descriptors_sorted_rad.columns:\n",
    "    descriptors_sorted_rad.rename(columns = {column: '{}_rad'.format(column)}, inplace = True)\n",
    "\n",
    "for column in descriptors_sorted_H.columns:\n",
    "    descriptors_sorted_H.rename(columns = {column: '{}_H'.format(column)}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = complete_dropped.index\n",
    "descriptors_sorted_rad = descriptors_sorted_rad.set_index(indices)\n",
    "descriptors_sorted_H = descriptors_sorted_H.set_index(indices)\n",
    "\n",
    "final_results = complete_dropped.join(descriptors_sorted_rad)\n",
    "final_results = final_results.join(descriptors_sorted_H)\n",
    "\n",
    "### Save final dataframe\n",
    "final_results.to_pickle('data_complete_w_descriptors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for GNNs\n",
    "We need the coordinates and nuclear charges of all atoms as input for the GNNs. Execution of this part might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kgcnn.utils.adj import coordinates_to_distancematrix, define_adjacency_from_distance, distance_to_gauss_basis, get_angle_indices, sort_edge_indices\n",
    "from ase.io import read\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data_ceomplete_final')\n",
    "pdb_start = data['pdb_file_start'].tolist()\n",
    "pdb_end = data['pdb_file_end'].tolist()\n",
    "del data\n",
    "\n",
    "all_nodes_start = []; all_nodes_end = []\n",
    "all_pos_start = []; all_pos_end = []\n",
    "all_n_start = []; all_n_end = []\n",
    "\n",
    "for file_start, file_end in zip(pdb_start, pdb_end):\n",
    "\n",
    "    mol_start = read(str(file_start.resolve()))\n",
    "    mol_end = read(str(file_end.resolve()))\n",
    "\n",
    "    an_start = mol_start.get_atomic_numbers()\n",
    "    an_end = mol_end.get_atomic_numbers()\n",
    "    n_start = mol_start.get_global_number_of_atoms()\n",
    "    n_end = mol_end.get_global_number_of_atoms()\n",
    "    pos_start = mol_start.positions\n",
    "    pos_end = mol_end.positions\n",
    "\n",
    "    # reacting H atom in its final position\n",
    "    nodes_start = np.concatenate((np.array([0]), an_start), axis=0)\n",
    "    pos_start_compl = np.concatenate((np.array([pos_end[0]]), pos_start), axis=0)\n",
    "\n",
    "    # reacting H atom in its initial position\n",
    "    nodes_end = np.concatenate((np.array([0]), an_end), axis=0)\n",
    "    pos_end_compl = np.concatenate((np.array([pos_start[0]]), pos_end), axis=0)\n",
    "\n",
    "    all_nodes_start.append(nodes_start); all_nodes_end.append(nodes_end)\n",
    "    all_pos_start.append(pos_start_compl); all_pos_end.append(pos_end_compl)\n",
    "    all_n_start.append(n_start); all_n_end.append(n_end)\n",
    "\n",
    "# create distcance matrices, adjacency matrices and edge indices, and expand distances in Gauss basis\n",
    "dist_mat_start = [coordinates_to_distancematrix(x) for x in all_pos_start]\n",
    "dist_mat_end = [coordinates_to_distancematrix(x) for x in all_pos_end]\n",
    "\n",
    "adj_mat_start = [define_adjacency_from_distance(x)[0] for x in dist_mat_start]\n",
    "adj_mat_end = [define_adjacency_from_distance(x)[0] for x in dist_mat_end]\n",
    "\n",
    "edge_idx_start = [define_adjacency_from_distance(x)[1] for x in dist_mat_start]\n",
    "edge_idx_end = [define_adjacency_from_distance(x)[1] for x in dist_mat_end]\n",
    "\n",
    "graph_input = pd.DataFrame(\n",
    "    zip(all_nodes_start, all_nodes_end, all_pos_start, all_pos_end, all_n_start, all_n_end,\n",
    "    edge_idx_start, edge_idx_end),\n",
    "    columns = [\n",
    "        'nodes_start', 'nodes_end', 'pos_start', 'pos_end', 'total_atoms_start', 'total_atoms_end',\n",
    "        'egde_idx_start', 'edge_idx_end'\n",
    "    ]\n",
    ")\n",
    "\n",
    "graph_input.to_pickle('graph_input_pickled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate final model\n",
    "Training and evaluation of the final model using K fold cross-validation will take around 8-10 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import K_fold_cross_validation, painn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "painn = painn()\n",
    "\n",
    "data = pd.read_pickle('data_complete_final')\n",
    "target = data['Ea'].to_numpy()\n",
    "descriptors = data[[\n",
    "    'translation', 'rad_BDE', 'H_BDE', 'max_spin_rad',\n",
    "    'mull_charge_rad', 'bur_vol_iso_rad', 'nBase_rad', 'SpMax_A_rad',\n",
    "    'ATSC2s_rad', 'ATSC1Z_rad', 'ATSC2i_rad', 'NdNH_rad', 'SMR_VSA4_rad',\n",
    "    'max_spin_H', 'mull_charge_H', 'bur_vol_iso_H', 'nBase_H', 'SpMax_A_H',\n",
    "    'ATSC2s_H', 'ATSC1Z_H', 'ATSC2i_H', 'GATS2dv_H', \n",
    "    'BCUTdv-1h_H', 'SMR_VSA4_H', 'VSA_EState7_H'\n",
    "]]\n",
    "del data\n",
    "\n",
    "descriptors = np.array(descriptors)\n",
    "\n",
    "graph_input = pd.read_pickle('graph_input_pickled')\n",
    "nodes = graph_input['nodes_start'].to_numpy()\n",
    "pos = graph_input['pos_start'].to_numpy()\n",
    "edge_idx = graph_input['egde_idx_start'].to_numpy()\n",
    "\n",
    "dist_mat_start = [coordinates_to_distancematrix(x) for x in pos]\n",
    "adj_mat_start = [define_adjacency_from_distance(x)[0] for x in dist_mat_start]\n",
    "edge_idx = [x if x[0,1]==1 else sort_edge_indices(np.concatenate([np.array([[0, 1], [1,0]]), x], axis=0)) for x in edge_idx]\n",
    "edge_idx = np.array(edge_idx)\n",
    "node_radical_index = [np.array([[0, 1]]) for _ in edge_idx]  \n",
    "edge_radical_index = [np.array([[0]]) for _ in node_radical_index]\n",
    "node_radical_index = np.array(node_radical_index)\n",
    "edge_radical_index = np.array(edge_radical_index)\n",
    "del graph_input\n",
    "\n",
    "input = [nodes, pos, edge_idx, node_radical_index, edge_radical_index, descriptors]\n",
    "\n",
    "train_mae_per_fold, val_mae_per_fold, \\\n",
    "    test_mae_per_fold = K_fold_cross_validation(input, target, 10, painn, batch_size=256, no_epochs=1000)\n",
    "\n",
    "results = pd.DataFrame(\n",
    "    zip(train_mae_per_fold, val_mae_per_fold, test_mae_per_fold),\n",
    "    columns=['train_maes', 'val_maes', 'test_maes']\n",
    ")\n",
    "\n",
    "results.to_csv('maes_per_fold_painn.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9810cd3559edf93e350c9ff83472b07b449301d37c5da9aaba0d8181928edeb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv_up': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
